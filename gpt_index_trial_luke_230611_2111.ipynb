{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryogrid/ryogridJupyterNotebooks/blob/master/gpt_index_trial_roberta_230611_2111.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faTMH-gFBiQg",
        "outputId": "5a2f5d93-2653-46a6-8d55-e40252ff3d5f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/gpt-index-trial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wsXXP7YBZiZ",
        "outputId": "da6b718c-5b58-4cbe-df96-d2532e03c06f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gpt-index-trial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTap3OYlgnd7",
        "outputId": "33d9ccf7-440d-45c4-a5ce-39799f509f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama-index\n",
            "langchain\n",
            "transformers\n",
            "sentence-transformers\n",
            "sentencepiece"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip3i8nw_DEdC",
        "outputId": "18971f62-7216-41b3-eb01-c022cfda04d1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting llama-index (from -r requirements.txt (line 1))\n",
            "  Downloading llama_index-0.6.23-py3-none-any.whl (493 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.5/493.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain (from -r requirements.txt (line 2))\n",
            "  Downloading langchain-0.0.196-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers (from -r requirements.txt (line 3))\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from -r requirements.txt (line 4))\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece (from -r requirements.txt (line 5))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Collecting sqlalchemy>=2.0.15 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading SQLAlchemy-2.0.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (8.2.2)\n",
            "Collecting openai>=0.26.4 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (1.5.3)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (1.26.15)\n",
            "Collecting fsspec>=2023.5.0 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect==0.8.0 (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions==4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index->-r requirements.txt (line 1)) (4.5.0)\n",
            "Collecting tiktoken (from llama-index->-r requirements.txt (line 1))\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect==0.8.0->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (6.0)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting langchainplus-sdk>=0.0.7 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading langchainplus_sdk-0.0.8-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.8.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain->-r requirements.txt (line 2))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers->-r requirements.txt (line 3))\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers->-r requirements.txt (line 3))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers->-r requirements.txt (line 3))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 3)) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 4)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 4)) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 4)) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 4)) (3.8.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2))\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2))\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2))\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 2))\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json->llama-index->-r requirements.txt (line 1))\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=2.0.15->llama-index->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (16.0.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r requirements.txt (line 4)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers->-r requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index->-r requirements.txt (line 1)) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers->-r requirements.txt (line 4)) (8.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index->-r requirements.txt (line 1)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers->-r requirements.txt (line 4)) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=39068728855dcc8bc42b887ec044e8ce9633067cc66bee1ffca3c9310c0a0bc8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, sqlalchemy, mypy-extensions, multidict, marshmallow, fsspec, frozenlist, async-timeout, yarl, typing-inspect, tiktoken, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, huggingface-hub, aiosignal, transformers, dataclasses-json, aiohttp, openai, langchain, llama-index, sentence-transformers\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.10\n",
            "    Uninstalling SQLAlchemy-2.0.10:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.10\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2023.4.0\n",
            "    Uninstalling fsspec-2023.4.0:\n",
            "      Successfully uninstalled fsspec-2023.4.0\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 fsspec-2023.6.0 huggingface-hub-0.15.1 langchain-0.0.196 langchainplus-sdk-0.0.8 llama-index-0.6.23 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openai-0.27.8 openapi-schema-pydantic-1.2.4 safetensors-0.3.1 sentence-transformers-2.2.2 sentencepiece-0.1.99 sqlalchemy-2.0.16 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.30.1 typing-inspect-0.8.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aN0y35XrA-aw"
      },
      "outputs": [],
      "source": [
        "from llama_index import LLMPredictor\n",
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Mapping, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "5Q8wFz_sB2WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikbyYf7GCoNp",
        "outputId": "b5a5d188-e9e8-48e1-d8ed-e257803dca88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gpt-index-trial\n",
            "requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "v5KVC4G5A-ay"
      },
      "outputs": [],
      "source": [
        "DEFAULT_PROMPT = \"\"\"\n",
        "文脈情報は以下です。\n",
        "---\n",
        "{context_str}\n",
        "---\n",
        "事前知識ではなく、文脈情報を参考に質問に答えてください。：{query_str}\n",
        "\"\"\"\n",
        "\n",
        "REFINE_PROMPT = \"\"\"\n",
        "質問は以下です。：{query_str}\n",
        "すでに答えの候補があります。：{existing_answer}\n",
        "必要な場合のみ、以下の情報を使って上の答えを改良することができます。文脈情報が有用でない場合は元の答えをそのまま返してください。\n",
        "---\n",
        "{context_msg}\n",
        "---\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ga0seAOEA-az"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, LukeForQuestionAnswering, T5Tokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Mizuiro-sakura/luke-japanese-large-finetuned-QA\")\n",
        "qa_model = LukeForQuestionAnswering.from_pretrained(\"Mizuiro-sakura/luke-japanese-large-finetuned-QA\")\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "    #qa_model = qa_model.to(\"cuda\")\n",
        "    #qa_model0 = qa_model0.to(\"cuda\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def inference_answer(prompt):\n",
        "    #print(prompt)\n",
        "    splited = prompt.split(\"文脈情報を参考に質問に答えてください。：\")\n",
        "    #print(len(splited))\n",
        "    #print(splited)\n",
        "    if len(splited) > 1:      \n",
        "      question = splited[1]\n",
        "      context = prompt.replace(question, \"\")\n",
        "    else:\n",
        "      # リファインする場合\n",
        "      #splited = prompt.split(\"必要な場合のみ、以下の文脈情報を使ってこの答えを改良することができます。\")\n",
        "      #question = splited[0] + \"必要な場合のみ、以下の文脈情報を使ってこの答えを改良することができます。\"\n",
        "      question = prompt\n",
        "      #context = prompt.replace(question,\"\")\n",
        "      context = \"\"\n",
        "\n",
        "    print(\"Q:\" + question + \"\\n\")\n",
        "    print(\"Context:\" + context + \"\\n\")\n",
        "    test_feature = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        max_length=318,\n",
        "        truncation=True\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        outputs = qa_model(torch.tensor([test_feature[\"input_ids\"]]))\n",
        "    #start_logits = outputs.start_logits.cpu().numpy()\n",
        "    #end_logits = outputs.end_logits.cpu().numpy()\n",
        "    start_logits = outputs.start_logits.to(qa_model.device).numpy()\n",
        "    end_logits = outputs.end_logits.to(qa_model.device).numpy()\n",
        "    answer_ids = test_feature[\"input_ids\"][np.argmax(start_logits):np.argmax(end_logits)+1]\n",
        "    return \"\".join(tokenizer.batch_decode(answer_ids))"
      ],
      "metadata": {
        "id": "PtBdkgDXy4PC"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSxv9SnkA-a0"
      },
      "outputs": [],
      "source": [
        "def generate(prompt):\n",
        "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "    n = len(token_ids[0])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = qa_model.generate(\n",
        "            token_ids.to(qa_model.device),\n",
        "            max_length=n+100,\n",
        "            min_length=n+2,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            bos_token_id=tokenizer.bos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    output = tokenizer.decode(output_ids.tolist()[0][n:])\n",
        "    return output.replace(\"</s>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from llama_index import LangchainEmbedding\n",
        "\n",
        "# load in HF embedding model from langchain\n",
        "embed_model = LangchainEmbedding(HuggingFaceEmbeddings(model_name=\"oshizo/sbert-jsnli-luke-japanese-base-lite\"))\n",
        "embed_model._langchain_embedding.client.max_seq_length = 256\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#  embed_model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "QAaqGwgm2f79"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "FzWPs3NPA-a1"
      },
      "outputs": [],
      "source": [
        "class CustomLLM(LLM):\n",
        "        \n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "    \n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        return inference_answer(prompt)\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {\"name\":\"custom\"}\n",
        "llm = CustomLLM()\n",
        "llm_predictor = LLMPredictor(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OH4pHOJzA-a2"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "import json\n",
        "url = \"https://ja.wikipedia.org/wiki/%E3%81%BC%E3%81%A3%E3%81%A1%E3%83%BB%E3%81%96%E3%83%BB%E3%82%8D%E3%81%A3%E3%81%8F!?action=cirrusdump\"\n",
        "with urllib.request.urlopen(url) as f:\n",
        "    data = f.read()\n",
        "text = json.loads(data)[0][\"_source\"][\"text\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from llama_index.callbacks.base import BaseCallbackHandler\n",
        "from llama_index.callbacks.schema import CBEventType\n",
        "\n",
        "class CustomCallbackHandler(BaseCallbackHandler):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        event_starts_to_ignore: Optional[List[CBEventType]] = None,\n",
        "        event_ends_to_ignore: Optional[List[CBEventType]] = None,\n",
        "        print_trace_on_end: bool = True,\n",
        "    ) -> None:\n",
        "        event_starts_to_ignore = event_starts_to_ignore if event_starts_to_ignore else []\n",
        "        event_ends_to_ignore = event_ends_to_ignore if event_ends_to_ignore else []\n",
        "        super().__init__(\n",
        "            event_starts_to_ignore=event_starts_to_ignore,\n",
        "            event_ends_to_ignore=event_ends_to_ignore,\n",
        "        )\n",
        "\n",
        "    def on_event_start(\n",
        "        self,\n",
        "        event_type: CBEventType,\n",
        "        payload: Optional[Dict[str, Any]] = None,\n",
        "        event_id: str = \"\",\n",
        "        **kwargs: Any\n",
        "    ) -> str:\n",
        "        print(f\"event_type = {event_type} (start)\")\n",
        "        return event_id\n",
        "\n",
        "    def on_event_end(\n",
        "        self,\n",
        "        event_type: CBEventType,\n",
        "        payload: Optional[Dict[str, Any]] = None,\n",
        "        event_id: str = \"\",\n",
        "        **kwargs: Any\n",
        "    ) -> None:\n",
        "        print(f\"event_type = {event_type} (end)\")\n",
        "     #   if event_type == CBEventType.RETRIEVE:\n",
        "     #     for selected_node in payload[\"nodes\"]:\n",
        "     #       print(selected_node)\n",
        "\n",
        "    def start_trace(self, trace_id: Optional[str] = None) -> None:\n",
        "        print(\"start_trace\")\n",
        "\n",
        "    def end_trace(\n",
        "        self,\n",
        "        trace_id: Optional[str] = None,\n",
        "        trace_map: Optional[Dict[str, List[str]]] = None,\n",
        "    ) -> None:\n",
        "        print(\"end_trace\")"
      ],
      "metadata": {
        "id": "zdl__NuCSpAS"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "rAiaHjWNA-a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd35e04-5884-4320-d263-84224c4e3318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = node_parsing (start)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = chunking (start)\n",
            "event_type = chunking (end)\n",
            "event_type = node_parsing (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "**********\n",
            "Trace: index_construction\n",
            "    |_node_parsing ->  0.261403 seconds\n",
            "      |_chunking ->  0.005337 seconds\n",
            "      |_chunking ->  0.001127 seconds\n",
            "      |_chunking ->  0.001097 seconds\n",
            "      |_chunking ->  0.001079 seconds\n",
            "      |_chunking ->  0.001117 seconds\n",
            "      |_chunking ->  0.001127 seconds\n",
            "      |_chunking ->  0.001074 seconds\n",
            "      |_chunking ->  0.001079 seconds\n",
            "      |_chunking ->  0.00113 seconds\n",
            "      |_chunking ->  0.001123 seconds\n",
            "      |_chunking ->  0.001069 seconds\n",
            "      |_chunking ->  0.001116 seconds\n",
            "      |_chunking ->  0.001144 seconds\n",
            "      |_chunking ->  0.001098 seconds\n",
            "      |_chunking ->  0.001101 seconds\n",
            "      |_chunking ->  0.0012 seconds\n",
            "      |_chunking ->  0.000815 seconds\n",
            "      |_chunking ->  0.0007 seconds\n",
            "      |_chunking ->  0.00069 seconds\n",
            "      |_chunking ->  0.000827 seconds\n",
            "      |_chunking ->  0.000747 seconds\n",
            "      |_chunking ->  0.000744 seconds\n",
            "      |_chunking ->  0.000725 seconds\n",
            "      |_chunking ->  0.00083 seconds\n",
            "      |_chunking ->  0.00068 seconds\n",
            "      |_chunking ->  0.000743 seconds\n",
            "      |_chunking ->  0.00085 seconds\n",
            "      |_chunking ->  0.000641 seconds\n",
            "      |_chunking ->  0.000768 seconds\n",
            "      |_chunking ->  0.000747 seconds\n",
            "      |_chunking ->  0.000741 seconds\n",
            "      |_chunking ->  0.000777 seconds\n",
            "      |_chunking ->  0.00068 seconds\n",
            "      |_chunking ->  0.000649 seconds\n",
            "      |_chunking ->  0.000757 seconds\n",
            "      |_chunking ->  0.00073 seconds\n",
            "      |_chunking ->  0.000684 seconds\n",
            "      |_chunking ->  0.000718 seconds\n",
            "      |_chunking ->  0.000636 seconds\n",
            "      |_chunking ->  0.000655 seconds\n",
            "      |_chunking ->  0.000984 seconds\n",
            "      |_chunking ->  0.000996 seconds\n",
            "      |_chunking ->  0.0009 seconds\n",
            "      |_chunking ->  0.000747 seconds\n",
            "      |_chunking ->  0.000699 seconds\n",
            "      |_chunking ->  0.000856 seconds\n",
            "      |_chunking ->  0.000847 seconds\n",
            "      |_chunking ->  0.000734 seconds\n",
            "      |_chunking ->  0.000661 seconds\n",
            "      |_chunking ->  0.000664 seconds\n",
            "      |_chunking ->  0.000673 seconds\n",
            "      |_chunking ->  0.000732 seconds\n",
            "      |_chunking ->  0.00076 seconds\n",
            "      |_chunking ->  0.000703 seconds\n",
            "      |_chunking ->  0.000629 seconds\n",
            "      |_chunking ->  0.000722 seconds\n",
            "      |_chunking ->  0.00071 seconds\n",
            "      |_chunking ->  0.000664 seconds\n",
            "      |_chunking ->  0.000638 seconds\n",
            "      |_chunking ->  0.000834 seconds\n",
            "      |_chunking ->  0.000802 seconds\n",
            "      |_chunking ->  0.000628 seconds\n",
            "      |_chunking ->  0.000729 seconds\n",
            "      |_chunking ->  0.000645 seconds\n",
            "      |_chunking ->  0.000671 seconds\n",
            "      |_chunking ->  0.000778 seconds\n",
            "      |_chunking ->  0.000659 seconds\n",
            "      |_chunking ->  0.000708 seconds\n",
            "      |_chunking ->  0.000714 seconds\n",
            "      |_chunking ->  0.000656 seconds\n",
            "      |_chunking ->  0.000639 seconds\n",
            "      |_chunking ->  0.000836 seconds\n",
            "      |_chunking ->  0.000768 seconds\n",
            "      |_chunking ->  0.000642 seconds\n",
            "      |_chunking ->  0.000768 seconds\n",
            "      |_chunking ->  0.000717 seconds\n",
            "      |_chunking ->  0.000725 seconds\n",
            "      |_chunking ->  0.000666 seconds\n",
            "      |_chunking ->  0.000644 seconds\n",
            "      |_chunking ->  0.000735 seconds\n",
            "      |_chunking ->  0.000692 seconds\n",
            "      |_chunking ->  0.000759 seconds\n",
            "      |_chunking ->  0.000739 seconds\n",
            "      |_chunking ->  0.000775 seconds\n",
            "      |_chunking ->  0.000658 seconds\n",
            "      |_chunking ->  0.000799 seconds\n",
            "      |_chunking ->  0.00082 seconds\n",
            "      |_chunking ->  0.000751 seconds\n",
            "      |_chunking ->  0.000737 seconds\n",
            "      |_chunking ->  0.000833 seconds\n",
            "      |_chunking ->  0.000753 seconds\n",
            "      |_chunking ->  0.000744 seconds\n",
            "      |_chunking ->  0.000738 seconds\n",
            "      |_chunking ->  0.00079 seconds\n",
            "      |_chunking ->  0.00095 seconds\n",
            "      |_chunking ->  0.001031 seconds\n",
            "      |_chunking ->  0.00103 seconds\n",
            "      |_chunking ->  0.001131 seconds\n",
            "      |_chunking ->  0.001136 seconds\n",
            "      |_chunking ->  0.001127 seconds\n",
            "      |_chunking ->  0.001094 seconds\n",
            "      |_chunking ->  0.001218 seconds\n",
            "      |_chunking ->  0.001215 seconds\n",
            "      |_chunking ->  0.001144 seconds\n",
            "      |_chunking ->  0.001262 seconds\n",
            "      |_chunking ->  0.001163 seconds\n",
            "      |_chunking ->  0.001103 seconds\n",
            "      |_chunking ->  0.001092 seconds\n",
            "      |_chunking ->  0.000905 seconds\n",
            "      |_chunking ->  0.000751 seconds\n",
            "      |_chunking ->  0.000748 seconds\n",
            "      |_chunking ->  0.000706 seconds\n",
            "      |_chunking ->  0.000885 seconds\n",
            "      |_chunking ->  0.000812 seconds\n",
            "      |_chunking ->  0.00092 seconds\n",
            "      |_chunking ->  0.000781 seconds\n",
            "      |_chunking ->  0.000829 seconds\n",
            "      |_chunking ->  0.000807 seconds\n",
            "      |_chunking ->  0.000795 seconds\n",
            "      |_chunking ->  0.000684 seconds\n",
            "      |_chunking ->  0.00076 seconds\n",
            "      |_chunking ->  0.000748 seconds\n",
            "      |_chunking ->  0.000758 seconds\n",
            "      |_chunking ->  0.000802 seconds\n",
            "      |_chunking ->  0.0008 seconds\n",
            "      |_chunking ->  0.000798 seconds\n",
            "      |_chunking ->  0.000863 seconds\n",
            "      |_chunking ->  0.000652 seconds\n",
            "      |_chunking ->  0.000789 seconds\n",
            "      |_chunking ->  0.000743 seconds\n",
            "      |_chunking ->  0.000709 seconds\n",
            "      |_chunking ->  0.000722 seconds\n",
            "      |_chunking ->  0.000828 seconds\n",
            "      |_chunking ->  0.000846 seconds\n",
            "      |_chunking ->  0.000753 seconds\n",
            "      |_chunking ->  0.000879 seconds\n",
            "      |_chunking ->  0.000804 seconds\n",
            "      |_chunking ->  0.00078 seconds\n",
            "      |_chunking ->  0.000804 seconds\n",
            "      |_chunking ->  0.00077 seconds\n",
            "      |_chunking ->  0.000704 seconds\n",
            "      |_chunking ->  0.000727 seconds\n",
            "      |_chunking ->  0.00071 seconds\n",
            "      |_chunking ->  0.00083 seconds\n",
            "      |_chunking ->  0.000816 seconds\n",
            "      |_chunking ->  0.00082 seconds\n",
            "      |_chunking ->  0.000844 seconds\n",
            "      |_chunking ->  0.000818 seconds\n",
            "      |_chunking ->  0.00085 seconds\n",
            "      |_chunking ->  0.000956 seconds\n",
            "      |_chunking ->  0.00075 seconds\n",
            "      |_chunking ->  0.000738 seconds\n",
            "      |_chunking ->  0.000782 seconds\n",
            "      |_chunking ->  0.000779 seconds\n",
            "      |_chunking ->  0.000972 seconds\n",
            "      |_chunking ->  0.00117 seconds\n",
            "      |_chunking ->  0.00116 seconds\n",
            "      |_chunking ->  0.001084 seconds\n",
            "      |_chunking ->  0.00117 seconds\n",
            "      |_chunking ->  0.001188 seconds\n",
            "      |_chunking ->  0.001137 seconds\n",
            "      |_chunking ->  0.001196 seconds\n",
            "      |_chunking ->  0.001129 seconds\n",
            "      |_chunking ->  0.001167 seconds\n",
            "      |_chunking ->  0.001245 seconds\n",
            "      |_chunking ->  0.00124 seconds\n",
            "      |_chunking ->  0.001122 seconds\n",
            "      |_chunking ->  0.001137 seconds\n",
            "      |_chunking ->  0.001152 seconds\n",
            "      |_chunking ->  0.00115 seconds\n",
            "      |_chunking ->  0.001266 seconds\n",
            "      |_chunking ->  0.001268 seconds\n",
            "      |_chunking ->  0.001202 seconds\n",
            "      |_chunking ->  0.001166 seconds\n",
            "      |_chunking ->  0.001126 seconds\n",
            "      |_chunking ->  0.000991 seconds\n",
            "      |_chunking ->  0.00091 seconds\n",
            "      |_chunking ->  0.000622 seconds\n",
            "      |_chunking ->  0.000818 seconds\n",
            "      |_chunking ->  0.000848 seconds\n",
            "      |_chunking ->  0.000873 seconds\n",
            "      |_chunking ->  0.000826 seconds\n",
            "      |_chunking ->  0.000691 seconds\n",
            "      |_chunking ->  0.00081 seconds\n",
            "      |_chunking ->  0.000835 seconds\n",
            "      |_chunking ->  0.000835 seconds\n",
            "      |_chunking ->  0.000865 seconds\n",
            "      |_chunking ->  0.000819 seconds\n",
            "      |_chunking ->  0.000782 seconds\n",
            "      |_chunking ->  0.000841 seconds\n",
            "      |_chunking ->  0.001583 seconds\n",
            "      |_chunking ->  0.000792 seconds\n",
            "      |_chunking ->  0.000839 seconds\n",
            "      |_chunking ->  0.000876 seconds\n",
            "      |_chunking ->  0.000831 seconds\n",
            "      |_chunking ->  0.000767 seconds\n",
            "      |_chunking ->  0.000763 seconds\n",
            "      |_chunking ->  0.001743 seconds\n",
            "      |_chunking ->  0.001624 seconds\n",
            "      |_chunking ->  0.001446 seconds\n",
            "      |_chunking ->  0.000934 seconds\n",
            "      |_chunking ->  0.001091 seconds\n",
            "      |_chunking ->  0.001537 seconds\n",
            "      |_chunking ->  0.001306 seconds\n",
            "      |_chunking ->  0.00091 seconds\n",
            "      |_chunking ->  0.000878 seconds\n",
            "      |_chunking ->  0.000842 seconds\n",
            "      |_chunking ->  0.000884 seconds\n",
            "      |_chunking ->  0.000816 seconds\n",
            "      |_chunking ->  0.000995 seconds\n",
            "      |_chunking ->  0.000923 seconds\n",
            "      |_chunking ->  0.00099 seconds\n",
            "      |_chunking ->  0.000933 seconds\n",
            "      |_chunking ->  0.00083 seconds\n",
            "      |_chunking ->  0.000869 seconds\n",
            "      |_chunking ->  0.000904 seconds\n",
            "      |_chunking ->  0.000879 seconds\n",
            "      |_chunking ->  0.000845 seconds\n",
            "      |_chunking ->  0.000868 seconds\n",
            "      |_chunking ->  0.000925 seconds\n",
            "      |_chunking ->  0.000906 seconds\n",
            "      |_chunking ->  0.000941 seconds\n",
            "      |_chunking ->  0.000901 seconds\n",
            "      |_chunking ->  0.000902 seconds\n",
            "      |_chunking ->  0.000889 seconds\n",
            "      |_chunking ->  0.000843 seconds\n",
            "      |_chunking ->  0.000853 seconds\n",
            "      |_chunking ->  0.000848 seconds\n",
            "      |_chunking ->  0.000863 seconds\n",
            "      |_chunking ->  0.000965 seconds\n",
            "      |_chunking ->  0.000915 seconds\n",
            "      |_chunking ->  0.000945 seconds\n",
            "      |_chunking ->  0.000953 seconds\n",
            "      |_chunking ->  0.000845 seconds\n",
            "      |_chunking ->  0.000857 seconds\n",
            "      |_chunking ->  0.000739 seconds\n",
            "      |_chunking ->  0.000875 seconds\n",
            "      |_chunking ->  0.000759 seconds\n",
            "      |_chunking ->  0.000837 seconds\n",
            "      |_chunking ->  0.000928 seconds\n",
            "      |_chunking ->  0.000973 seconds\n",
            "      |_chunking ->  0.00084 seconds\n",
            "      |_chunking ->  0.000813 seconds\n",
            "      |_chunking ->  0.0008 seconds\n",
            "      |_chunking ->  0.000769 seconds\n",
            "      |_chunking ->  0.000775 seconds\n",
            "      |_chunking ->  0.000812 seconds\n",
            "      |_chunking ->  0.000766 seconds\n",
            "      |_chunking ->  0.000728 seconds\n",
            "      |_chunking ->  0.000752 seconds\n",
            "      |_chunking ->  0.000765 seconds\n",
            "      |_chunking ->  0.000686 seconds\n",
            "      |_chunking ->  0.000819 seconds\n",
            "    |_embedding ->  0.101603 seconds\n",
            "    |_embedding ->  0.086784 seconds\n",
            "    |_embedding ->  0.094111 seconds\n",
            "    |_embedding ->  0.069603 seconds\n",
            "    |_embedding ->  0.068223 seconds\n",
            "    |_embedding ->  0.070794 seconds\n",
            "    |_embedding ->  0.067449 seconds\n",
            "    |_embedding ->  0.069562 seconds\n",
            "    |_embedding ->  0.06973 seconds\n",
            "    |_embedding ->  0.069738 seconds\n",
            "    |_embedding ->  0.068889 seconds\n",
            "    |_embedding ->  0.07201 seconds\n",
            "    |_embedding ->  0.06944 seconds\n",
            "    |_embedding ->  0.064974 seconds\n",
            "    |_embedding ->  0.069517 seconds\n",
            "    |_embedding ->  0.070387 seconds\n",
            "    |_embedding ->  0.068719 seconds\n",
            "    |_embedding ->  0.07291 seconds\n",
            "    |_embedding ->  0.069994 seconds\n",
            "    |_embedding ->  0.078732 seconds\n",
            "    |_embedding ->  0.079373 seconds\n",
            "    |_embedding ->  0.080874 seconds\n",
            "    |_embedding ->  0.077128 seconds\n",
            "    |_embedding ->  0.077848 seconds\n",
            "    |_embedding ->  0.082873 seconds\n",
            "    |_embedding ->  0.030148 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        }
      ],
      "source": [
        "#from llama_index import GPTVectorStoreIndex\n",
        "from llama_index import LLMPredictor, ServiceContext, GPTSimpleKeywordTableIndex, SimpleKeywordTableIndex, VectorStoreIndex\n",
        "from llama_index.readers.schema.base import Document\n",
        "from llama_index.indices.list.base import ListRetrieverMode\n",
        "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
        "from llama_index.indices.service_context import set_global_service_context\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.indices.query.response_synthesis import ResponseSynthesizer\n",
        "from llama_index.indices.response.type import ResponseMode\n",
        "from llama_index.indices.prompt_helper import PromptHelper\n",
        "\n",
        "documents = []\n",
        "for i in range(0, len(text), 200):\n",
        "    documents.append(Document(text[i:i+200]))\n",
        "    if i != 0:\n",
        "        documents.append(Document(text[i-100:i+100]))\n",
        "\n",
        "#llm_predictor   = LLMPredictor(llm=llm)\n",
        "llama_debug_handler = LlamaDebugHandler()\n",
        "custom_callback_handler = CustomCallbackHandler()\n",
        "callback_manager = CallbackManager([\n",
        "    llama_debug_handler\n",
        "    , custom_callback_handler\n",
        "])\n",
        "#callback_manager = CallbackManager([llama_debug_handler])\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, callback_manager=callback_manager, prompt_helper=PromptHelper.from_llm_metadata(llm_predictor.get_llm_metadata()), embed_model=embed_model)\n",
        "#set_global_service_context(service_context=service_context)\n",
        "#service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
        "\n",
        "# インデックスの作成\n",
        "#index = GPTVectorStoreIndex.from_documents(documents, llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "#index = GPTSimpleKeywordTableIndex.from_documents(documents, service_context=service_context, llm_predictor=llm_predictor, embed_model=embed_model)\n",
        "index = VectorStoreIndex.from_documents(documents, service_context=service_context, llm_predictor=llm_predictor, embed_model=embed_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uxC5eYFA-a4",
        "outputId": "48095a76-bc86-4c27-e2c5-3e416152beba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.prompts.base import Prompt\n",
        "from llama_index.prompts.prompts import RefinePrompt, QuestionAnswerPrompt\n",
        "refine_prompt = RefinePrompt(REFINE_PROMPT)\n",
        "default_prompt = QuestionAnswerPrompt(DEFAULT_PROMPT)"
      ],
      "metadata": {
        "id": "r0CWwaO958vE"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "hpKB77twA-a7",
        "outputId": "2ecf7139-6e5d-4924-ba65-ba879aa97ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = query (start)\n",
            "event_type = retrieve (start)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = retrieve (end)\n",
            "event_type = synthesize (start)\n",
            "event_type = llm (start)\n",
            "Q:虹夏ちゃんのお姉さんの仕事は？\n",
            "\n",
            "\n",
            "Context:\n",
            "文脈情報は以下です。\n",
            "---\n",
            "へと呼び方を改める。同じ学校のひとりのことを気にかけるが、一方でひとりの性格などを歯に衣着せず述べ、無自覚のうちに精神的ダメージを与えることもしばしば。そのストレートな物言いは虹夏（アニメではリョウ）に「ナチュラルに鬼畜」と評された。両親は共に公務員であり、曰く「母親はしっかりしている」とのこと。 伊地知 星歌（いじち せいか） 声 - 内田真礼 虹夏の姉で、ライブハウス「STARRY」の店長。ク\n",
            "とは公にはされていない。 伊地知 虹夏（いじち にじか） 声 - 鈴代紗弓 誕生日：5月29日 / 身長：154cm / 体重：48kg / 血液型：A型 ドラム担当。下北沢高校2年→3年。明るく世話焼きなバンドのリーダーでまとめ役。また、本作におけるツッコミ役。水玉のリボンがトレードマークで、たいてい身体のどこかに身につけている。姉の星歌がライブハウスの店長を務める元バンドマンであることの影響か\n",
            "ドを始めようと考え、虹夏と喜多に付き合ってもらってギターを購入。以後は学校でもバイト先でも、ひとりにギターを教えてもらおうとまとわりつく。 日向 恵恋奈（ひなた えれな） 作中2年目から登場。16歳で通信制高校在籍の1年生。猫々と同じくSTARRYの新人バイト。一人称は「えれ」。もとは結束バンドと対バンした地下アイドルグループ「天使のキューティクル」のメンバーで、「ラファエル」を名乗っていた。 天\n",
            "らは「だめな大人」の一人と思われている。 上村 レコーディングエンジニア。朗らかで話し好きな女性。隙あらば料金を上乗せしようとする。 リナ 星歌の元バンドメンバーでドラマー。現在はスタジオミュージシャン。小学生だった虹夏にねだられ、最初にドラムを教えた人物。 星歌に呼ばれ、アルバムレコーディングにてプレッシャーのため実力が出せない虹夏にアドバイスを与えた。 後藤 直樹（ごとう なおき） 声 - 間\n",
            "に「ナチュラルに鬼畜」と評された。両親は共に公務員であり、曰く「母親はしっかりしている」とのこと。 伊地知 星歌（いじち せいか） 声 - 内田真礼 虹夏の姉で、ライブハウス「STARRY」の店長。クリスマスイブが誕生日で、初登場時29歳。作中1年目に30歳を迎え、2年目に31歳を迎えた。かつては自身もバンドマン（ギタリスト）で、その実力はレーベルからスカウトの話が来るほどだった。口では虹夏や結束\n",
            "---\n",
            "事前知識ではなく、文脈情報を参考に質問に答えてください。：\n",
            "\n",
            "event_type = llm (end)\n",
            "event_type = synthesize (end)\n",
            "event_type = query (end)\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.282182 seconds\n",
            "      |_retrieve ->  0.10139 seconds\n",
            "        |_embedding ->  0.021854 seconds\n",
            "      |_synthesize ->  3.180254 seconds\n",
            "        |_llm ->  3.176905 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ハウスの店長を務める元バンドマンであること'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "#query_engine = index.as_query_engine()\n",
        "\n",
        "'''\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index, \n",
        "    similarity_top_k=2,\n",
        ")\n",
        "'''\n",
        "\n",
        "retriever = index.as_retriever(retriever_mode='embedding',  similarity_top_k=5)\n",
        "#retriever = index.as_retriever(retriever_mode='simple')\n",
        "#retriever = index.as_retriever()\n",
        "#retriever = index.as_retriever(retriever_mode='embedding')\n",
        "\n",
        "#response_synthesizer = ResponseSynthesizer.from_args(response_mode = ResponseMode.REFINE ,service_context=service_context, text_qa_template=default_prompt, refine_template=refine_prompt, callback_manager=callback_manager, verbose=True)\n",
        "#response_synthesizer = ResponseSynthesizer.from_args(response_mode = ResponseMode.COMPACT ,service_context=service_context, text_qa_template=default_prompt, refine_template=refine_prompt, callback_manager=callback_manager, verbose=True)\n",
        "response_synthesizer = ResponseSynthesizer.from_args(response_mode = ResponseMode.SIMPLE_SUMMARIZE, service_context=service_context, text_qa_template=default_prompt, refine_template=refine_prompt, callback_manager=callback_manager, verbose=True)\n",
        "query_engine = RetrieverQueryEngine(retriever=retriever, response_synthesizer=response_synthesizer, callback_manager=callback_manager)\n",
        "'''\n",
        "query_engine = index.as_query_engine(\n",
        "    retriever_mode=ListRetrieverMode.EMBEDDING\n",
        "    , similarity_top_k=3\n",
        ")\n",
        "'''\n",
        "\n",
        "response = query_engine.query(\n",
        "    \"虹夏ちゃんのお姉さんの仕事は？\"\n",
        ")\n",
        "'''\n",
        "response = index.query(\n",
        "    \"虹夏ちゃんのお姉さんの仕事は？\", \n",
        "    mode=\"embedding\", \n",
        "    verbose=True, \n",
        "    embed_model=embed_model,\n",
        "    text_qa_template=default_prompt,\n",
        "    refine_template=refine_prompt,\n",
        "    similarity_top_k=2\n",
        ")\n",
        "'''\n",
        "response.response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "eKmcx9eRA-a7",
        "outputId": "948fb6bf-ba8c-431a-a3e2-cef802023a1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = query (start)\n",
            "event_type = retrieve (start)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = retrieve (end)\n",
            "event_type = synthesize (start)\n",
            "event_type = llm (start)\n",
            "Q:後藤ひとりがギターに熱中するようになった理由になった出来事は？\n",
            "\n",
            "\n",
            "Context:\n",
            "文脈情報は以下です。\n",
            "---\n",
            "ているといち早く気付いている。 先述の性格に加え、運動や勉強など取り柄と言えるものがないというコンプレックスを持っていたところ、中学1年時に暗い学生時代から一転してスターとなったバンドマンのインタビューを目にしたことで、父親の直樹から借りたギターに没頭する。高校入学後はギターを持って公園にいたところを虹夏から声を掛けられ、逃げた郁代の代わりとして臨時で結束バンドのギターを務める運びになり、その流れ\n",
            "島淳司 誕生日：10月16日 / 血液型：O型 後藤姉妹の父。洋楽趣味でギターを所持しており、ひとりは当初そのギターを借りて使用していた。ひとりの音楽活動については「洋楽にハマってほしかった」としながらも好意的に見守っており、文化祭ライブでギターを壊したことを告白された際には「ライブ中に壊すなんて興奮しちゃうなあ」と話すなどロックに対する理解も高いが、当時中1だったひとりに貸し与えたギターは、虹夏\n",
            "バンドに厳しくあたることもあるが、実際には誰よりも結束バンドのことを親身に考えており、特に虹夏の事で一喜一憂する。ぶっきらぼうな態度や性格に似合わず可愛い物が好きで、ぬいぐるみを抱いていたりメイド服や女子学生服などを鑑賞用に所持していたりする。また涙もろい。 ひとりのギターヒーローとしての正体まではわからなかったものの、ひとりのギタリストとしての才能にもいち早く気づいており、ひとりを中心にした結束\n",
            "うになった。また、きくりとの出会いや様々なライブを経て、人間的に成長していく。 毎日6時間以上ギターの練習を約3年間欠かさずやってきたため、プロレベルと称されるほど個人としての演奏技術はずば抜けて高い。一方で人前が苦手な性格や、他人と合わせながら演奏する経験の不足からバンド演奏では実力を十分に発揮できず、時折その片鱗を覗かせるに留まっている。結束バンドでは作詞を請け負っているものの、青春に関連した\n",
            "ーを目にしたことで、父親の直樹から借りたギターに没頭する。高校入学後はギターを持って公園にいたところを虹夏から声を掛けられ、逃げた郁代の代わりとして臨時で結束バンドのギターを務める運びになり、その流れで結束バンドに加入した。その後は改めて加入した郁代とギターの指導による師弟関係を結び、また虹夏やリョウとの交流を経て当初は自分のためとしてやっていた音楽の道を、結束バンドの皆と一緒に進みたいと思えるよ\n",
            "---\n",
            "事前知識ではなく、文脈情報を参考に質問に答えてください。：\n",
            "\n",
            "event_type = llm (end)\n",
            "event_type = synthesize (end)\n",
            "event_type = query (end)\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  1.875626 seconds\n",
            "      |_retrieve ->  0.099614 seconds\n",
            "        |_embedding ->  0.020427 seconds\n",
            "      |_synthesize ->  1.774806 seconds\n",
            "        |_llm ->  1.77038 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'夏バンド'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 101
        }
      ],
      "source": [
        "response = query_engine.query(\n",
        "    \"後藤ひとりがギターに熱中するようになった理由になった出来事は？\", \n",
        ")\n",
        "\n",
        "response.response"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"ギターヒーローの正体は誰？\", \n",
        ")\n",
        "response.response\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "B_YMQ3NkJtyF",
        "outputId": "65c30923-92f7-4888-e5db-f92f0139d43b"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = query (start)\n",
            "event_type = retrieve (start)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = retrieve (end)\n",
            "event_type = synthesize (start)\n",
            "event_type = llm (start)\n",
            "Q:ギターヒーローの正体は誰？\n",
            "\n",
            "\n",
            "Context:\n",
            "文脈情報は以下です。\n",
            "---\n",
            "女子学生服などを鑑賞用に所持していたりする。また涙もろい。 ひとりのギターヒーローとしての正体まではわからなかったものの、ひとりのギタリストとしての才能にもいち早く気づいており、ひとりを中心にした結束バンドの動画を密かに撮影するなど注目している。 PAさん 声 - 小岩井ことり ミステリアスな風貌をしたSTARRYの女性PAエンジニア。最終学歴は高校中退。プライベートでは「音戯 アルト（おとぎ ア\n",
            "束バンドの取材にSTARRYを訪れ、演奏の癖からひとりとギターヒーローが同一人物であることを見抜く。ギターヒーローの動画に注目する1人で、その実力を既にプロでも通用すると評価している。一方で結束バンドについては「高校生にしたらレベルはまぁ高いと思う」としつつも、その活動姿勢を「ガチじゃない」「本気でプロを目指しているバンドに見えない」と評し「こんなところでうだうだやってると才能腐っちゃいますよ」と\n",
            "の子だったが、担当編集から「もっとガチにした方がいい」と指摘を受けて修正していき、たちまち現在の設定となった。 ギターヒーロー ひとりが動画投稿サイトで活動する際に用いるハンドルネーム。ひとりが普段人前に出せないが内面に抱えている承認欲求の象徴とも言えるもので、現実のひとりからはかけ離れた「バスケ部エースの彼氏持ち」や「ラインの友達数は1000人超え」といった設定がつけられている。顔を隠すアングル\n",
            "スまで結束バンドを取材しに来る。ぽいずん♡やみはそこでのひとりによる演奏のクセを聞いて、ひとりがギターヒーローであることを確信し皆にバラした。ひとり（ギターヒーロー）の腕前なら確実にプロとしても通用すると絶賛した一方、他の結束バンドのメンバーに関しては「”ガチ”じゃない」「本気でプロを目指してやっているようには見えない」と酷評。「ちゃんとしたバンドに入った方がいい」と、ひとりだけを引き抜いて音楽業\n",
            "バンドに厳しくあたることもあるが、実際には誰よりも結束バンドのことを親身に考えており、特に虹夏の事で一喜一憂する。ぶっきらぼうな態度や性格に似合わず可愛い物が好きで、ぬいぐるみを抱いていたりメイド服や女子学生服などを鑑賞用に所持していたりする。また涙もろい。 ひとりのギターヒーローとしての正体まではわからなかったものの、ひとりのギタリストとしての才能にもいち早く気づいており、ひとりを中心にした結束\n",
            "---\n",
            "事前知識ではなく、文脈情報を参考に質問に答えてください。：\n",
            "\n",
            "event_type = llm (end)\n",
            "event_type = synthesize (end)\n",
            "event_type = query (end)\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  1.878657 seconds\n",
            "      |_retrieve ->  0.075142 seconds\n",
            "        |_embedding ->  0.018693 seconds\n",
            "      |_synthesize ->  1.803204 seconds\n",
            "        |_llm ->  1.800362 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ひとりのギターヒーローとしての正体まではわからなかったものの、ひとりのギタリストとしての才能にもいち早く気づいており、ひとりを中心にした結束バンドの動画を密かに撮影するなど注目している。PAさん声-小岩井ことりミステリアスな風貌をしたSTARRYの女性PAエンジニア。最終学歴は高校中退。プライベートでは「音戯アルト(おとぎア束バンドの取材にSTARRYを訪れ、演奏の癖からひとりとギターヒーローが同一人物であることを'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"喜多郁代の両親の仕事は？\", \n",
        ")\n",
        "response.response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "bHrMXH4iLNx1",
        "outputId": "9559c6ad-a097-4ebb-d292-9b803bf95503"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = query (start)\n",
            "event_type = retrieve (start)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = retrieve (end)\n",
            "event_type = synthesize (start)\n",
            "event_type = llm (start)\n",
            "Q:喜多郁代の両親の仕事は？\n",
            "\n",
            "\n",
            "Context:\n",
            "文脈情報は以下です。\n",
            "---\n",
            " 声 - 末柄里恵 誕生日：4月3日 /血液型：A型 後藤姉妹の母。ひとり及びふたりと同じくピンク色の髪をしている。人見知りなひとりを何かと心配している。喜多が家に来たときは場を盛り上げようと、喜んで学生コスプレまでした（21年ぶりに制服を着たとのこと）。結束バンドを夫と一緒になって応援しており、そのためにも学生に扮したことがある。 後藤 ふたり（ごとう ふたり） 声 - 和多田美咲 誕生日：7月\n",
            "たお金を与えている。 目はいつも何か（髪の毛や、ふたりの手など）に隠されていて描かれていない。 会社では窓際族と説明しているが、自宅は金沢八景（横浜市）の戸建てである。 後藤 美智代（ごとう みちよ） 声 - 末柄里恵 誕生日：4月3日 /血液型：A型 後藤姉妹の母。ひとり及びふたりと同じくピンク色の髪をしている。人見知りなひとりを何かと心配している。喜多が家に来たときは場を盛り上げようと、喜んで\n",
            " 小岩井ことり 後藤家で飼われている犬。直樹によって洋楽由来で名付けられた。初対面の相手にもよくなつく。 リョウの両親 職業は病院の経営。リョウを溺愛しているが、本人からはその愛情を疎ましがられてもいる。だが、バンドのために車を出してもらうなど、リョウは時々頼ってもいる。 結束バンドのファン1号・2号 声 - 市ノ瀬加那（1号）、島袋美由利（2号） ひとりときくりの即興ライブを見て結束バンドのライ\n",
            "ン。小学生だった虹夏にねだられ、最初にドラムを教えた人物。 星歌に呼ばれ、アルバムレコーディングにてプレッシャーのため実力が出せない虹夏にアドバイスを与えた。 後藤 直樹（ごとう なおき） 声 - 間島淳司 誕生日：10月16日 / 血液型：O型 後藤姉妹の父。洋楽趣味でギターを所持しており、ひとりは当初そのギターを借りて使用していた。ひとりの音楽活動については「洋楽にハマってほしかった」としなが\n",
            "には虹夏の依頼で撮影にも協力している。具体的な名前については作中で言及されず、2人のうちロングヘアの女性が「1号さん」、ミディアムヘアの女性が「2号さん」と呼ばれる。 佐々木 次子（ささき つぐこ） 誕生日：4月2日 / 血液型：O型 作中2年目から登場する、ひとりと喜多のクラスメイト。喜多とは中学から5年連続同じクラスの腐れ縁で、「さっつー」のニックネームで呼ばれる。また、ひとりからは「ささささ\n",
            "---\n",
            "事前知識ではなく、文脈情報を参考に質問に答えてください。：\n",
            "\n",
            "event_type = llm (end)\n",
            "event_type = synthesize (end)\n",
            "event_type = query (end)\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  1.880131 seconds\n",
            "      |_retrieve ->  0.076962 seconds\n",
            "        |_embedding ->  0.021379 seconds\n",
            "      |_synthesize ->  1.802474 seconds\n",
            "        |_llm ->  1.800188 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'。リ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\n",
        "    \"ひとりが動画投稿する時のハンドルネームは？\", \n",
        ")\n",
        "response.response\n",
        "#contextStr = \"\"\n",
        "#for node in llama_debug_handler.get_event_pairs(CBEventType.RETRIEVE):\n",
        "#  for nodeWithScore in node[1].payload[\"nodes\"]:\n",
        "#    contextStr += nodeWithScore.node.text\n",
        "#inference_answer(\"ひとりが動画投稿する時のハンドルネームは？\", contextStr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "Ao0CSbi7Kugo",
        "outputId": "8845695c-e04f-4d67-878a-12ea4adde053"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start_trace\n",
            "event_type = query (start)\n",
            "event_type = retrieve (start)\n",
            "event_type = embedding (start)\n",
            "event_type = embedding (end)\n",
            "event_type = retrieve (end)\n",
            "event_type = synthesize (start)\n",
            "event_type = llm (start)\n",
            "Q:ひとりが動画投稿する時のハンドルネームは？\n",
            "\n",
            "\n",
            "Context:\n",
            "文脈情報は以下です。\n",
            "---\n",
            "の子だったが、担当編集から「もっとガチにした方がいい」と指摘を受けて修正していき、たちまち現在の設定となった。 ギターヒーロー ひとりが動画投稿サイトで活動する際に用いるハンドルネーム。ひとりが普段人前に出せないが内面に抱えている承認欲求の象徴とも言えるもので、現実のひとりからはかけ離れた「バスケ部エースの彼氏持ち」や「ラインの友達数は1000人超え」といった設定がつけられている。顔を隠すアングル\n",
            "ると、顔を上げ表情も整えればアイドル事務所も狙えるほどのルックスだが、その顔は10秒と持たない。そして自己評価が極めて低く、ひとり自身は外見やスタイルを全く意識していない。 初期設定では普通の可愛い女の子だったが、担当編集から「もっとガチにした方がいい」と指摘を受けて修正していき、たちまち現在の設定となった。 ギターヒーロー ひとりが動画投稿サイトで活動する際に用いるハンドルネーム。ひとりが普段人\n",
            "前に出せないが内面に抱えている承認欲求の象徴とも言えるもので、現実のひとりからはかけ離れた「バスケ部エースの彼氏持ち」や「ラインの友達数は1000人超え」といった設定がつけられている。顔を隠すアングルから撮影された人気バンドのカバー動画を投稿しており、投稿サイトでの登録者数は8万人近く。ぽいずん♡やみのような音楽関係者からも注目を集めている。動画は直樹によって収益化されており、文化祭ライブで壊れた\n",
            "バンドの動画を密かに撮影するなど注目している。 PAさん 声 - 小岩井ことり ミステリアスな風貌をしたSTARRYの女性PAエンジニア。最終学歴は高校中退。プライベートでは「音戯 アルト（おとぎ アルト）」の名前でバーチャルYouTuberとして活動しており、ゲーム実況動画などを公開している。具体的な名前については作中で言及されず、主に「PAさん」と呼ばれる。 大山 猫々（おおやま ねね） 作中\n",
            "から撮影された人気バンドのカバー動画を投稿しており、投稿サイトでの登録者数は8万人近く。ぽいずん♡やみのような音楽関係者からも注目を集めている。動画は直樹によって収益化されており、文化祭ライブで壊れたギターを新調する費用等に充てられた。 ライブでの現在の自身の実力ではギターヒーローの名前でファンを集めても満足させるだけの演奏ができないというひとりの考えから、正体が「結束バンドの後藤ひとり」であるこ\n",
            "---\n",
            "事前知識ではなく、文脈情報を参考に質問に答えてください。：\n",
            "\n",
            "event_type = llm (end)\n",
            "event_type = synthesize (end)\n",
            "event_type = query (end)\n",
            "**********\n",
            "Trace: query\n",
            "    |_query ->  3.063568 seconds\n",
            "      |_retrieve ->  0.105168 seconds\n",
            "        |_embedding ->  0.026519 seconds\n",
            "      |_synthesize ->  2.957146 seconds\n",
            "        |_llm ->  2.954788 seconds\n",
            "**********\n",
            "end_trace\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ギターヒーローひとりが'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.11"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "7f654d66d151deba9457e27d06730dde595b930a8810fc70e1b9eaf1b703f5dc"
      }
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
