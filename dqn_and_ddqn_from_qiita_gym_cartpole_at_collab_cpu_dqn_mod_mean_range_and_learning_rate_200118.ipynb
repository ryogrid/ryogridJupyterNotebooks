{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_and_ddqn_from_qiita_gym_cartpole_at_collab_cpu_dqn_mod_mean_range_and_learning_rate_200118.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNZssRmMlsD0jeY4ODBiSN1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryogrid/ryogridJupyterNotebooks/blob/master/dqn_and_ddqn_from_qiita_gym_cartpole_at_collab_cpu_dqn_mod_mean_range_and_learning_rate_200118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWnKUAbLAXmF",
        "colab_type": "text"
      },
      "source": [
        "https://qiita.com/sugulu/items/bc7c70e6658f204f85f9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81VK6pAQAN0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f2abb913-aa8b-4a6f-9e6a-17cab109cb0e"
      },
      "source": [
        "# coding:utf-8\n",
        "# [0]必要なライブラリのインポート\n",
        "import gym  # 倒立振子(cartpole)の実行環境\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from collections import deque\n",
        "from gym import wrappers  # gymの画像保存\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# [1]損失関数の定義\n",
        "# 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py\n",
        "def huberloss(y_true, y_pred):\n",
        "    err = y_true - y_pred\n",
        "    cond = K.abs(err) < 1.0\n",
        "    L2 = 0.5 * K.square(err)\n",
        "    L1 = (K.abs(err) - 0.5)\n",
        "    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(\n",
        "    return K.mean(loss)\n",
        "\n",
        "\n",
        "# [2]Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "class QNetwork:\n",
        "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size))\n",
        "        self.model.add(Dense(hidden_size, activation='relu'))\n",
        "        self.model.add(Dense(action_size, activation='linear'))\n",
        "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\n",
        "        # self.model.compile(loss='mse', optimizer=self.optimizer)\n",
        "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
        "\n",
        "    # 重みの学習\n",
        "    def replay(self, memory, batch_size, gamma, targetQN):\n",
        "        inputs = np.zeros((batch_size, 4))\n",
        "        targets = np.zeros((batch_size, 2))\n",
        "        mini_batch = memory.sample(batch_size)\n",
        "\n",
        "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
        "            inputs[i:i + 1] = state_b\n",
        "            target = reward_b\n",
        "\n",
        "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
        "                # 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）\n",
        "                retmainQs = self.model.predict(next_state_b)[0]\n",
        "                next_action = np.argmax(retmainQs)  # 最大の報酬を返す行動を選択する\n",
        "                target = reward_b + gamma * targetQN.model.predict(next_state_b)[0][next_action]\n",
        "\n",
        "            targets[i] = self.model.predict(state_b)    # Qネットワークの出力\n",
        "            targets[i][action_b] = target               # 教師信号\n",
        "\n",
        "        # shiglayさんよりアドバイスいただき、for文の外へ修正しました\n",
        "        self.model.fit(inputs, targets, epochs=1, verbose=0)  # epochsは訓練データの反復回数、verbose=0は表示なしの設定\n",
        "\n",
        "\n",
        "# [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス\n",
        "class Memory:\n",
        "    def __init__(self, max_size=1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "        return [self.buffer[ii] for ii in idx]\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# [4]カートの状態に応じて、行動を決定するクラス\n",
        "# アドバイスいただき、引数にtargetQNを使用していたのをmainQNに修正しました\n",
        "class Actor:\n",
        "    def get_action(self, state, episode, mainQN):   # [C]ｔ＋１での行動を返す\n",
        "        # 徐々に最適行動のみをとる、ε-greedy法\n",
        "        epsilon = 0.001 + 0.9 / (1.0+episode)\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            retTargetQs = mainQN.model.predict(state)[0]\n",
        "            action = np.argmax(retTargetQs)  # 最大の報酬を返す行動を選択する\n",
        "\n",
        "        else:\n",
        "            action = np.random.choice([0, 1])  # ランダムに行動する\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "# [5] メイン関数開始----------------------------------------------------\n",
        "# [5.1] 初期設定--------------------------------------------------------\n",
        "DQN_MODE = 1    # 1がDQN、0がDDQNです\n",
        "LENDER_MODE = 0 # 0は学習後も描画なし、1は学習終了後に描画する\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "num_episodes = 1000  # 総試行回数\n",
        "max_number_of_steps = 200  # 1試行のstep数\n",
        "goal_average_reward = 195  # この報酬を超えると学習終了\n",
        "num_consecutive_iterations = 10  # 学習完了評価の平均計算を行う試行回数\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n",
        "gamma = 0.99    # 割引係数\n",
        "islearned = 0  # 学習が終わったフラグ\n",
        "isrender = 0  # 描画フラグ\n",
        "# ---\n",
        "hidden_size = 16               # Q-networkの隠れ層のニューロンの数\n",
        "learning_rate = 0.0001 #0.00001         # Q-networkの学習係数\n",
        "memory_size = 10000            # バッファーメモリの大きさ\n",
        "batch_size = 32                # Q-networkを更新するバッチの大記載\n",
        "\n",
        "# [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------\n",
        "mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)     # メインのQネットワーク\n",
        "targetQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)   # 価値を計算するQネットワーク\n",
        "# plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化\n",
        "memory = Memory(max_size=memory_size)\n",
        "actor = Actor()\n",
        "\n",
        "# [5.3]メインルーチン--------------------------------------------------------\n",
        "for episode in range(num_episodes):  # 試行数分繰り返す\n",
        "    env.reset()  # cartPoleの環境初期化\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())  # 1step目は適当な行動をとる\n",
        "    state = np.reshape(state, [1, 4])   # list型のstateを、1行4列の行列に変換\n",
        "    episode_reward = 0\n",
        "\n",
        "\n",
        "    # 2018.05.16\n",
        "    # skanmeraさんより間違いを修正いただきました\n",
        "    # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "    # ↓\n",
        "    targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "    for t in range(max_number_of_steps + 1):  # 1試行のループ\n",
        "        if (islearned == 1) and LENDER_MODE:  # 学習終了したらcartPoleを描画する\n",
        "            env.render()\n",
        "            time.sleep(0.1)\n",
        "            print(state[0, 0])  # カートのx位置を出力するならコメントはずす\n",
        "\n",
        "        action = actor.get_action(state, episode, mainQN)   # 時刻tでの行動を決定する\n",
        "        next_state, reward, done, info = env.step(action)   # 行動a_tの実行による、s_{t+1}, _R{t}を計算する\n",
        "        next_state = np.reshape(next_state, [1, 4])     # list型のstateを、1行4列の行列に変換\n",
        "\n",
        "        # 報酬を設定し、与える\n",
        "        if done:\n",
        "            next_state = np.zeros(state.shape)  # 次の状態s_{t+1}はない\n",
        "            if t < 195:\n",
        "                reward = -1  # 報酬クリッピング、報酬は1, 0, -1に固定\n",
        "            else:\n",
        "                reward = 1  # 立ったまま195step超えて終了時は報酬\n",
        "        else:\n",
        "            reward = 0  # 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）\n",
        "\n",
        "        episode_reward += 1 # reward  # 合計報酬を更新\n",
        "\n",
        "        memory.add((state, action, reward, next_state))     # メモリの更新する\n",
        "        state = next_state  # 状態更新\n",
        "\n",
        "\n",
        "        # Qネットワークの重みを学習・更新する replay\n",
        "        if (memory.len() > batch_size) and not islearned:\n",
        "            mainQN.replay(memory, batch_size, gamma, targetQN)\n",
        "\n",
        "        if DQN_MODE:\n",
        "        # 2018.06.12\n",
        "        # shiglayさんさんより間違いを修正いただきました\n",
        "        # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "        # ↓\n",
        "            targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "        # 1施行終了時の処理\n",
        "        if done:\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n",
        "            print('%d Episode finished after %f time steps / episode_reward %f / mean of last 20 episode %f' % (episode, t + 1, episode_reward, total_reward_vec[0:19].mean()))\n",
        "            break\n",
        "\n",
        "    # 複数施行の平均報酬で終了を判断\n",
        "    if total_reward_vec[0:19].mean() >= goal_average_reward:\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        islearned = 1\n",
        "        if isrender == 0:   # 学習済みフラグを更新\n",
        "            isrender = 1\n",
        "\n",
        "            # env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合\n",
        "            # 10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
        "            # if episode>10:\n",
        "            #    if isrender == 0:\n",
        "            #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
        "            #        isrender = 1\n",
        "            #    islearned=1;"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-fa7d260edb9d>:21: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "0 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 1.000000\n",
            "1 Episode finished after 19.000000 time steps / episode_reward 19.000000 / mean of last 20 episode 2.900000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "2 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 4.000000\n",
            "3 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 5.000000\n",
            "4 Episode finished after 15.000000 time steps / episode_reward 15.000000 / mean of last 20 episode 6.500000\n",
            "5 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 7.400000\n",
            "6 Episode finished after 14.000000 time steps / episode_reward 14.000000 / mean of last 20 episode 8.800000\n",
            "7 Episode finished after 13.000000 time steps / episode_reward 13.000000 / mean of last 20 episode 10.100000\n",
            "8 Episode finished after 15.000000 time steps / episode_reward 15.000000 / mean of last 20 episode 11.600000\n",
            "9 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 12.500000\n",
            "10 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 12.400000\n",
            "11 Episode finished after 28.000000 time steps / episode_reward 28.000000 / mean of last 20 episode 13.300000\n",
            "12 Episode finished after 16.000000 time steps / episode_reward 16.000000 / mean of last 20 episode 13.800000\n",
            "13 Episode finished after 32.000000 time steps / episode_reward 32.000000 / mean of last 20 episode 16.000000\n",
            "14 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 15.400000\n",
            "15 Episode finished after 14.000000 time steps / episode_reward 14.000000 / mean of last 20 episode 15.900000\n",
            "16 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 15.400000\n",
            "17 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 14.900000\n",
            "18 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 14.200000\n",
            "19 Episode finished after 29.000000 time steps / episode_reward 29.000000 / mean of last 20 episode 16.200000\n",
            "20 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 16.100000\n",
            "21 Episode finished after 14.000000 time steps / episode_reward 14.000000 / mean of last 20 episode 14.700000\n",
            "22 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 14.200000\n",
            "23 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 12.000000\n",
            "24 Episode finished after 15.000000 time steps / episode_reward 15.000000 / mean of last 20 episode 12.600000\n",
            "25 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 11.900000\n",
            "26 Episode finished after 15.000000 time steps / episode_reward 15.000000 / mean of last 20 episode 12.500000\n",
            "27 Episode finished after 21.000000 time steps / episode_reward 21.000000 / mean of last 20 episode 13.800000\n",
            "28 Episode finished after 16.000000 time steps / episode_reward 16.000000 / mean of last 20 episode 14.600000\n",
            "29 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 12.600000\n",
            "30 Episode finished after 19.000000 time steps / episode_reward 19.000000 / mean of last 20 episode 13.700000\n",
            "31 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 13.400000\n",
            "32 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 13.100000\n",
            "33 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 12.900000\n",
            "34 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 12.200000\n",
            "35 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 12.300000\n",
            "36 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.700000\n",
            "37 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 10.500000\n",
            "38 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 9.800000\n",
            "39 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 9.700000\n",
            "40 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 8.600000\n",
            "41 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.400000\n",
            "42 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 8.600000\n",
            "43 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 8.600000\n",
            "44 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 8.600000\n",
            "45 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.700000\n",
            "46 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.700000\n",
            "47 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.700000\n",
            "48 Episode finished after 25.000000 time steps / episode_reward 25.000000 / mean of last 20 episode 10.300000\n",
            "49 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 10.300000\n",
            "50 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 10.400000\n",
            "51 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 10.300000\n",
            "52 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 10.300000\n",
            "53 Episode finished after 23.000000 time steps / episode_reward 23.000000 / mean of last 20 episode 11.800000\n",
            "54 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.900000\n",
            "55 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.900000\n",
            "56 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 11.800000\n",
            "57 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 11.700000\n",
            "58 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 10.300000\n",
            "59 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 10.400000\n",
            "60 Episode finished after 22.000000 time steps / episode_reward 22.000000 / mean of last 20 episode 11.700000\n",
            "61 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 11.600000\n",
            "62 Episode finished after 23.000000 time steps / episode_reward 23.000000 / mean of last 20 episode 12.900000\n",
            "63 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.500000\n",
            "64 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 11.300000\n",
            "65 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 11.400000\n",
            "66 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 11.300000\n",
            "67 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 11.300000\n",
            "68 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.100000\n",
            "69 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 11.100000\n",
            "70 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 9.700000\n",
            "71 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 9.900000\n",
            "72 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.500000\n",
            "73 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.500000\n",
            "74 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 8.500000\n",
            "75 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 8.500000\n",
            "76 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 8.800000\n",
            "77 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.900000\n",
            "78 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 8.900000\n",
            "79 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 9.100000\n",
            "80 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 9.200000\n",
            "81 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 9.300000\n",
            "82 Episode finished after 9.000000 time steps / episode_reward 9.000000 / mean of last 20 episode 9.300000\n",
            "83 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 9.200000\n",
            "84 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 9.600000\n",
            "85 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 9.600000\n",
            "86 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 9.700000\n",
            "87 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 9.900000\n",
            "88 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 9.800000\n",
            "89 Episode finished after 12.000000 time steps / episode_reward 12.000000 / mean of last 20 episode 9.900000\n",
            "90 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 9.700000\n",
            "91 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 9.400000\n",
            "92 Episode finished after 11.000000 time steps / episode_reward 11.000000 / mean of last 20 episode 9.600000\n",
            "93 Episode finished after 7.000000 time steps / episode_reward 7.000000 / mean of last 20 episode 9.500000\n",
            "94 Episode finished after 10.000000 time steps / episode_reward 10.000000 / mean of last 20 episode 9.400000\n",
            "95 Episode finished after 8.000000 time steps / episode_reward 8.000000 / mean of last 20 episode 9.200000\n",
            "96 Episode finished after 14.000000 time steps / episode_reward 14.000000 / mean of last 20 episode 9.500000\n",
            "97 Episode finished after 20.000000 time steps / episode_reward 20.000000 / mean of last 20 episode 10.400000\n",
            "98 Episode finished after 20.000000 time steps / episode_reward 20.000000 / mean of last 20 episode 11.600000\n",
            "99 Episode finished after 16.000000 time steps / episode_reward 16.000000 / mean of last 20 episode 12.000000\n",
            "100 Episode finished after 18.000000 time steps / episode_reward 18.000000 / mean of last 20 episode 13.100000\n",
            "101 Episode finished after 34.000000 time steps / episode_reward 34.000000 / mean of last 20 episode 15.800000\n",
            "102 Episode finished after 17.000000 time steps / episode_reward 17.000000 / mean of last 20 episode 16.400000\n",
            "103 Episode finished after 19.000000 time steps / episode_reward 19.000000 / mean of last 20 episode 17.600000\n",
            "104 Episode finished after 20.000000 time steps / episode_reward 20.000000 / mean of last 20 episode 18.600000\n",
            "105 Episode finished after 17.000000 time steps / episode_reward 17.000000 / mean of last 20 episode 19.500000\n",
            "106 Episode finished after 20.000000 time steps / episode_reward 20.000000 / mean of last 20 episode 20.100000\n",
            "107 Episode finished after 31.000000 time steps / episode_reward 31.000000 / mean of last 20 episode 21.200000\n",
            "108 Episode finished after 15.000000 time steps / episode_reward 15.000000 / mean of last 20 episode 20.700000\n",
            "109 Episode finished after 71.000000 time steps / episode_reward 71.000000 / mean of last 20 episode 26.200000\n",
            "110 Episode finished after 33.000000 time steps / episode_reward 33.000000 / mean of last 20 episode 27.700000\n",
            "111 Episode finished after 23.000000 time steps / episode_reward 23.000000 / mean of last 20 episode 26.600000\n",
            "112 Episode finished after 36.000000 time steps / episode_reward 36.000000 / mean of last 20 episode 28.500000\n",
            "113 Episode finished after 53.000000 time steps / episode_reward 53.000000 / mean of last 20 episode 31.900000\n",
            "114 Episode finished after 36.000000 time steps / episode_reward 36.000000 / mean of last 20 episode 33.500000\n",
            "115 Episode finished after 29.000000 time steps / episode_reward 29.000000 / mean of last 20 episode 34.700000\n",
            "116 Episode finished after 150.000000 time steps / episode_reward 150.000000 / mean of last 20 episode 47.700000\n",
            "117 Episode finished after 32.000000 time steps / episode_reward 32.000000 / mean of last 20 episode 47.800000\n",
            "118 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 66.200000\n",
            "119 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 79.000000\n",
            "120 Episode finished after 26.000000 time steps / episode_reward 26.000000 / mean of last 20 episode 78.300000\n",
            "121 Episode finished after 117.000000 time steps / episode_reward 117.000000 / mean of last 20 episode 87.700000\n",
            "122 Episode finished after 48.000000 time steps / episode_reward 48.000000 / mean of last 20 episode 88.900000\n",
            "123 Episode finished after 26.000000 time steps / episode_reward 26.000000 / mean of last 20 episode 86.200000\n",
            "124 Episode finished after 62.000000 time steps / episode_reward 62.000000 / mean of last 20 episode 88.800000\n",
            "125 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 105.800000\n",
            "126 Episode finished after 74.000000 time steps / episode_reward 74.000000 / mean of last 20 episode 98.200000\n",
            "127 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 114.900000\n",
            "128 Episode finished after 188.000000 time steps / episode_reward 188.000000 / mean of last 20 episode 113.800000\n",
            "129 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 113.800000\n",
            "130 Episode finished after 195.000000 time steps / episode_reward 195.000000 / mean of last 20 episode 130.700000\n",
            "131 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 138.900000\n",
            "132 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 154.000000\n",
            "133 Episode finished after 174.000000 time steps / episode_reward 174.000000 / mean of last 20 episode 168.800000\n",
            "134 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.500000\n",
            "135 Episode finished after 48.000000 time steps / episode_reward 48.000000 / mean of last 20 episode 167.400000\n",
            "136 Episode finished after 83.000000 time steps / episode_reward 83.000000 / mean of last 20 episode 168.300000\n",
            "137 Episode finished after 157.000000 time steps / episode_reward 157.000000 / mean of last 20 episode 164.100000\n",
            "138 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 165.200000\n",
            "139 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 165.200000\n",
            "140 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 165.600000\n",
            "141 Episode finished after 58.000000 time steps / episode_reward 58.000000 / mean of last 20 episode 151.500000\n",
            "142 Episode finished after 73.000000 time steps / episode_reward 73.000000 / mean of last 20 episode 138.900000\n",
            "143 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 141.400000\n",
            "144 Episode finished after 95.000000 time steps / episode_reward 95.000000 / mean of last 20 episode 131.000000\n",
            "145 Episode finished after 77.000000 time steps / episode_reward 77.000000 / mean of last 20 episode 133.900000\n",
            "146 Episode finished after 103.000000 time steps / episode_reward 103.000000 / mean of last 20 episode 135.900000\n",
            "147 Episode finished after 97.000000 time steps / episode_reward 97.000000 / mean of last 20 episode 129.900000\n",
            "148 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 129.900000\n",
            "149 Episode finished after 117.000000 time steps / episode_reward 117.000000 / mean of last 20 episode 121.700000\n",
            "150 Episode finished after 101.000000 time steps / episode_reward 101.000000 / mean of last 20 episode 111.900000\n",
            "151 Episode finished after 156.000000 time steps / episode_reward 156.000000 / mean of last 20 episode 121.700000\n",
            "152 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 134.300000\n",
            "153 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 134.300000\n",
            "154 Episode finished after 75.000000 time steps / episode_reward 75.000000 / mean of last 20 episode 132.300000\n",
            "155 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 144.500000\n",
            "156 Episode finished after 70.000000 time steps / episode_reward 70.000000 / mean of last 20 episode 141.200000\n",
            "157 Episode finished after 66.000000 time steps / episode_reward 66.000000 / mean of last 20 episode 138.100000\n",
            "158 Episode finished after 75.000000 time steps / episode_reward 75.000000 / mean of last 20 episode 125.700000\n",
            "159 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 133.900000\n",
            "160 Episode finished after 107.000000 time steps / episode_reward 107.000000 / mean of last 20 episode 134.500000\n",
            "161 Episode finished after 105.000000 time steps / episode_reward 105.000000 / mean of last 20 episode 129.400000\n",
            "162 Episode finished after 104.000000 time steps / episode_reward 104.000000 / mean of last 20 episode 119.900000\n",
            "163 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 119.900000\n",
            "164 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 132.300000\n",
            "165 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 132.300000\n",
            "166 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 145.200000\n",
            "167 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 158.500000\n",
            "168 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 170.900000\n",
            "169 Episode finished after 89.000000 time steps / episode_reward 89.000000 / mean of last 20 episode 159.900000\n",
            "170 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 169.100000\n",
            "171 Episode finished after 128.000000 time steps / episode_reward 128.000000 / mean of last 20 episode 171.400000\n",
            "172 Episode finished after 128.000000 time steps / episode_reward 128.000000 / mean of last 20 episode 173.800000\n",
            "173 Episode finished after 170.000000 time steps / episode_reward 170.000000 / mean of last 20 episode 170.900000\n",
            "174 Episode finished after 169.000000 time steps / episode_reward 169.000000 / mean of last 20 episode 167.900000\n",
            "175 Episode finished after 114.000000 time steps / episode_reward 114.000000 / mean of last 20 episode 159.400000\n",
            "176 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 159.400000\n",
            "177 Episode finished after 96.000000 time steps / episode_reward 96.000000 / mean of last 20 episode 149.100000\n",
            "178 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 149.100000\n",
            "179 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 160.100000\n",
            "180 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 160.100000\n",
            "181 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 167.200000\n",
            "182 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 174.300000\n",
            "183 Episode finished after 103.000000 time steps / episode_reward 103.000000 / mean of last 20 episode 167.600000\n",
            "184 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 170.600000\n",
            "185 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.100000\n",
            "186 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.100000\n",
            "187 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 189.400000\n",
            "188 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 189.400000\n",
            "189 Episode finished after 94.000000 time steps / episode_reward 94.000000 / mean of last 20 episode 178.900000\n",
            "190 Episode finished after 169.000000 time steps / episode_reward 169.000000 / mean of last 20 episode 175.900000\n",
            "191 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 175.900000\n",
            "192 Episode finished after 177.000000 time steps / episode_reward 177.000000 / mean of last 20 episode 173.700000\n",
            "193 Episode finished after 91.000000 time steps / episode_reward 91.000000 / mean of last 20 episode 172.500000\n",
            "194 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.500000\n",
            "195 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.500000\n",
            "196 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.500000\n",
            "197 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.500000\n",
            "198 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.500000\n",
            "199 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 183.000000\n",
            "200 Episode finished after 185.000000 time steps / episode_reward 185.000000 / mean of last 20 episode 184.600000\n",
            "201 Episode finished after 163.000000 time steps / episode_reward 163.000000 / mean of last 20 episode 181.000000\n",
            "202 Episode finished after 181.000000 time steps / episode_reward 181.000000 / mean of last 20 episode 181.400000\n",
            "203 Episode finished after 135.000000 time steps / episode_reward 135.000000 / mean of last 20 episode 185.800000\n",
            "204 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "205 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "206 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "207 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "208 Episode finished after 159.000000 time steps / episode_reward 159.000000 / mean of last 20 episode 181.800000\n",
            "209 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.800000\n",
            "210 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 183.200000\n",
            "211 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.800000\n",
            "212 Episode finished after 173.000000 time steps / episode_reward 173.000000 / mean of last 20 episode 186.000000\n",
            "213 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.400000\n",
            "214 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.400000\n",
            "215 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.400000\n",
            "216 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.400000\n",
            "217 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.400000\n",
            "218 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 196.400000\n",
            "Episode 218 train agent successfuly!\n",
            "219 Episode finished after 198.000000 time steps / episode_reward 198.000000 / mean of last 20 episode 196.300000\n",
            "Episode 219 train agent successfuly!\n",
            "220 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 196.300000\n",
            "Episode 220 train agent successfuly!\n",
            "221 Episode finished after 120.000000 time steps / episode_reward 120.000000 / mean of last 20 episode 188.400000\n",
            "222 Episode finished after 143.000000 time steps / episode_reward 143.000000 / mean of last 20 episode 185.400000\n",
            "223 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.400000\n",
            "224 Episode finished after 156.000000 time steps / episode_reward 156.000000 / mean of last 20 episode 181.100000\n",
            "225 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.100000\n",
            "226 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.100000\n",
            "227 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.100000\n",
            "228 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.100000\n",
            "229 Episode finished after 115.000000 time steps / episode_reward 115.000000 / mean of last 20 episode 172.800000\n",
            "230 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 172.800000\n",
            "231 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 180.700000\n",
            "232 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.300000\n",
            "233 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.300000\n",
            "234 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 190.600000\n",
            "235 Episode finished after 104.000000 time steps / episode_reward 104.000000 / mean of last 20 episode 181.100000\n",
            "236 Episode finished after 186.000000 time steps / episode_reward 186.000000 / mean of last 20 episode 179.800000\n",
            "237 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.800000\n",
            "238 Episode finished after 178.000000 time steps / episode_reward 178.000000 / mean of last 20 episode 177.700000\n",
            "239 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.100000\n",
            "240 Episode finished after 121.000000 time steps / episode_reward 121.000000 / mean of last 20 episode 178.300000\n",
            "241 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 178.300000\n",
            "242 Episode finished after 154.000000 time steps / episode_reward 154.000000 / mean of last 20 episode 173.800000\n",
            "243 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 173.800000\n",
            "244 Episode finished after 183.000000 time steps / episode_reward 183.000000 / mean of last 20 episode 172.200000\n",
            "245 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "246 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 183.000000\n",
            "247 Episode finished after 139.000000 time steps / episode_reward 139.000000 / mean of last 20 episode 177.000000\n",
            "248 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.100000\n",
            "249 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.100000\n",
            "250 Episode finished after 124.000000 time steps / episode_reward 124.000000 / mean of last 20 episode 179.400000\n",
            "251 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.400000\n",
            "252 Episode finished after 181.000000 time steps / episode_reward 181.000000 / mean of last 20 episode 182.100000\n",
            "253 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.100000\n",
            "254 Episode finished after 162.000000 time steps / episode_reward 162.000000 / mean of last 20 episode 180.000000\n",
            "255 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 180.000000\n",
            "256 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 180.000000\n",
            "257 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.000000\n",
            "258 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.000000\n",
            "259 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.000000\n",
            "260 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 193.500000\n",
            "261 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 193.500000\n",
            "262 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 195.300000\n",
            "Episode 262 train agent successfuly!\n",
            "263 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 195.300000\n",
            "Episode 263 train agent successfuly!\n",
            "264 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 264 train agent successfuly!\n",
            "265 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 265 train agent successfuly!\n",
            "266 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 266 train agent successfuly!\n",
            "267 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 267 train agent successfuly!\n",
            "268 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 268 train agent successfuly!\n",
            "269 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 199.000000\n",
            "Episode 269 train agent successfuly!\n",
            "270 Episode finished after 125.000000 time steps / episode_reward 125.000000 / mean of last 20 episode 191.600000\n",
            "271 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 191.600000\n",
            "272 Episode finished after 117.000000 time steps / episode_reward 117.000000 / mean of last 20 episode 183.400000\n",
            "273 Episode finished after 182.000000 time steps / episode_reward 182.000000 / mean of last 20 episode 181.700000\n",
            "274 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "275 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "276 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "277 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "278 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "279 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 181.700000\n",
            "280 Episode finished after 172.000000 time steps / episode_reward 172.000000 / mean of last 20 episode 186.400000\n",
            "281 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 186.400000\n",
            "282 Episode finished after 178.000000 time steps / episode_reward 178.000000 / mean of last 20 episode 192.500000\n",
            "283 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 194.200000\n",
            "284 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 194.200000\n",
            "285 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 194.200000\n",
            "286 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 194.200000\n",
            "287 Episode finished after 115.000000 time steps / episode_reward 115.000000 / mean of last 20 episode 185.800000\n",
            "288 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "289 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.800000\n",
            "290 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 188.500000\n",
            "291 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 188.500000\n",
            "292 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 190.600000\n",
            "293 Episode finished after 121.000000 time steps / episode_reward 121.000000 / mean of last 20 episode 182.800000\n",
            "294 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.800000\n",
            "295 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.800000\n",
            "296 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.800000\n",
            "297 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 191.200000\n",
            "298 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 191.200000\n",
            "299 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 191.200000\n",
            "300 Episode finished after 181.000000 time steps / episode_reward 181.000000 / mean of last 20 episode 189.400000\n",
            "301 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 189.400000\n",
            "302 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 189.400000\n",
            "303 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 197.200000\n",
            "Episode 303 train agent successfuly!\n",
            "304 Episode finished after 116.000000 time steps / episode_reward 116.000000 / mean of last 20 episode 188.900000\n",
            "305 Episode finished after 164.000000 time steps / episode_reward 164.000000 / mean of last 20 episode 185.400000\n",
            "306 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.400000\n",
            "307 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 185.400000\n",
            "308 Episode finished after 121.000000 time steps / episode_reward 121.000000 / mean of last 20 episode 177.600000\n",
            "309 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 177.600000\n",
            "310 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.400000\n",
            "311 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.400000\n",
            "312 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.400000\n",
            "313 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 179.400000\n",
            "314 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 187.700000\n",
            "315 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 191.200000\n",
            "316 Episode finished after 136.000000 time steps / episode_reward 136.000000 / mean of last 20 episode 184.900000\n",
            "317 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 184.900000\n",
            "318 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 192.700000\n",
            "319 Episode finished after 173.000000 time steps / episode_reward 173.000000 / mean of last 20 episode 190.100000\n",
            "320 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 190.100000\n",
            "321 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 190.100000\n",
            "322 Episode finished after 175.000000 time steps / episode_reward 175.000000 / mean of last 20 episode 187.700000\n",
            "323 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 187.700000\n",
            "324 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 187.700000\n",
            "325 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 187.700000\n",
            "326 Episode finished after 149.000000 time steps / episode_reward 149.000000 / mean of last 20 episode 189.000000\n",
            "327 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 189.000000\n",
            "328 Episode finished after 110.000000 time steps / episode_reward 110.000000 / mean of last 20 episode 180.100000\n",
            "329 Episode finished after 199.000000 time steps / episode_reward 199.000000 / mean of last 20 episode 182.700000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fa7d260edb9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;31m# targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# ↓\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# 1施行終了時の処理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    512\u001b[0m                 \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m         \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2730\u001b[0m             \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2731\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2732\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}