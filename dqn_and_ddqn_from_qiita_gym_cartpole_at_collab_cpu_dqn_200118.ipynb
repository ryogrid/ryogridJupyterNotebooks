{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dqn_and_ddqn_from_qiita_gym_cartpole_at_collab_cpu_dqn_200118.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP5zXBWPwwGSY4JitdJPU6B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryogrid/ryogridJupyterNotebooks/blob/master/dqn_and_ddqn_from_qiita_gym_cartpole_at_collab_cpu_dqn_200118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWnKUAbLAXmF",
        "colab_type": "text"
      },
      "source": [
        "https://qiita.com/sugulu/items/bc7c70e6658f204f85f9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81VK6pAQAN0h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "692025b9-83bc-4c0d-a785-273786d8c5d9"
      },
      "source": [
        "# coding:utf-8\n",
        "# [0]必要なライブラリのインポート\n",
        "import gym  # 倒立振子(cartpole)の実行環境\n",
        "import numpy as np\n",
        "import time\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import plot_model\n",
        "from collections import deque\n",
        "from gym import wrappers  # gymの画像保存\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# [1]損失関数の定義\n",
        "# 損失関数にhuber関数を使用します 参考https://github.com/jaara/AI-blog/blob/master/CartPole-DQN.py\n",
        "def huberloss(y_true, y_pred):\n",
        "    err = y_true - y_pred\n",
        "    cond = K.abs(err) < 1.0\n",
        "    L2 = 0.5 * K.square(err)\n",
        "    L1 = (K.abs(err) - 0.5)\n",
        "    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(\n",
        "    return K.mean(loss)\n",
        "\n",
        "\n",
        "# [2]Q関数をディープラーニングのネットワークをクラスとして定義\n",
        "class QNetwork:\n",
        "    def __init__(self, learning_rate=0.01, state_size=4, action_size=2, hidden_size=10):\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(hidden_size, activation='relu', input_dim=state_size))\n",
        "        self.model.add(Dense(hidden_size, activation='relu'))\n",
        "        self.model.add(Dense(action_size, activation='linear'))\n",
        "        self.optimizer = Adam(lr=learning_rate)  # 誤差を減らす学習方法はAdam\n",
        "        # self.model.compile(loss='mse', optimizer=self.optimizer)\n",
        "        self.model.compile(loss=huberloss, optimizer=self.optimizer)\n",
        "\n",
        "    # 重みの学習\n",
        "    def replay(self, memory, batch_size, gamma, targetQN):\n",
        "        inputs = np.zeros((batch_size, 4))\n",
        "        targets = np.zeros((batch_size, 2))\n",
        "        mini_batch = memory.sample(batch_size)\n",
        "\n",
        "        for i, (state_b, action_b, reward_b, next_state_b) in enumerate(mini_batch):\n",
        "            inputs[i:i + 1] = state_b\n",
        "            target = reward_b\n",
        "\n",
        "            if not (next_state_b == np.zeros(state_b.shape)).all(axis=1):\n",
        "                # 価値計算（DDQNにも対応できるように、行動決定のQネットワークと価値観数のQネットワークは分離）\n",
        "                retmainQs = self.model.predict(next_state_b)[0]\n",
        "                next_action = np.argmax(retmainQs)  # 最大の報酬を返す行動を選択する\n",
        "                target = reward_b + gamma * targetQN.model.predict(next_state_b)[0][next_action]\n",
        "\n",
        "            targets[i] = self.model.predict(state_b)    # Qネットワークの出力\n",
        "            targets[i][action_b] = target               # 教師信号\n",
        "\n",
        "        # shiglayさんよりアドバイスいただき、for文の外へ修正しました\n",
        "        self.model.fit(inputs, targets, epochs=1, verbose=0)  # epochsは訓練データの反復回数、verbose=0は表示なしの設定\n",
        "\n",
        "\n",
        "# [3]Experience ReplayとFixed Target Q-Networkを実現するメモリクラス\n",
        "class Memory:\n",
        "    def __init__(self, max_size=1000):\n",
        "        self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idx = np.random.choice(np.arange(len(self.buffer)), size=batch_size, replace=False)\n",
        "        return [self.buffer[ii] for ii in idx]\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "# [4]カートの状態に応じて、行動を決定するクラス\n",
        "# アドバイスいただき、引数にtargetQNを使用していたのをmainQNに修正しました\n",
        "class Actor:\n",
        "    def get_action(self, state, episode, mainQN):   # [C]ｔ＋１での行動を返す\n",
        "        # 徐々に最適行動のみをとる、ε-greedy法\n",
        "        epsilon = 0.001 + 0.9 / (1.0+episode)\n",
        "\n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            retTargetQs = mainQN.model.predict(state)[0]\n",
        "            action = np.argmax(retTargetQs)  # 最大の報酬を返す行動を選択する\n",
        "\n",
        "        else:\n",
        "            action = np.random.choice([0, 1])  # ランダムに行動する\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "# [5] メイン関数開始----------------------------------------------------\n",
        "# [5.1] 初期設定--------------------------------------------------------\n",
        "DQN_MODE = 1    # 1がDQN、0がDDQNです\n",
        "LENDER_MODE = 0 # 0は学習後も描画なし、1は学習終了後に描画する\n",
        "\n",
        "env = gym.make('CartPole-v0')\n",
        "num_episodes = 299  # 総試行回数\n",
        "max_number_of_steps = 200  # 1試行のstep数\n",
        "goal_average_reward = 195  # この報酬を超えると学習終了\n",
        "num_consecutive_iterations = 10  # 学習完了評価の平均計算を行う試行回数\n",
        "total_reward_vec = np.zeros(num_consecutive_iterations)  # 各試行の報酬を格納\n",
        "gamma = 0.99    # 割引係数\n",
        "islearned = 0  # 学習が終わったフラグ\n",
        "isrender = 0  # 描画フラグ\n",
        "# ---\n",
        "hidden_size = 16               # Q-networkの隠れ層のニューロンの数\n",
        "learning_rate = 0.00001         # Q-networkの学習係数\n",
        "memory_size = 10000            # バッファーメモリの大きさ\n",
        "batch_size = 32                # Q-networkを更新するバッチの大記載\n",
        "\n",
        "# [5.2]Qネットワークとメモリ、Actorの生成--------------------------------------------------------\n",
        "mainQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)     # メインのQネットワーク\n",
        "targetQN = QNetwork(hidden_size=hidden_size, learning_rate=learning_rate)   # 価値を計算するQネットワーク\n",
        "# plot_model(mainQN.model, to_file='Qnetwork.png', show_shapes=True)        # Qネットワークの可視化\n",
        "memory = Memory(max_size=memory_size)\n",
        "actor = Actor()\n",
        "\n",
        "# [5.3]メインルーチン--------------------------------------------------------\n",
        "for episode in range(num_episodes):  # 試行数分繰り返す\n",
        "    env.reset()  # cartPoleの環境初期化\n",
        "    state, reward, done, _ = env.step(env.action_space.sample())  # 1step目は適当な行動をとる\n",
        "    state = np.reshape(state, [1, 4])   # list型のstateを、1行4列の行列に変換\n",
        "    episode_reward = 0\n",
        "\n",
        "\n",
        "    # 2018.05.16\n",
        "    # skanmeraさんより間違いを修正いただきました\n",
        "    # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "    # ↓\n",
        "    targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "    for t in range(max_number_of_steps + 1):  # 1試行のループ\n",
        "        if (islearned == 1) and LENDER_MODE:  # 学習終了したらcartPoleを描画する\n",
        "            env.render()\n",
        "            time.sleep(0.1)\n",
        "            print(state[0, 0])  # カートのx位置を出力するならコメントはずす\n",
        "\n",
        "        action = actor.get_action(state, episode, mainQN)   # 時刻tでの行動を決定する\n",
        "        next_state, reward, done, info = env.step(action)   # 行動a_tの実行による、s_{t+1}, _R{t}を計算する\n",
        "        next_state = np.reshape(next_state, [1, 4])     # list型のstateを、1行4列の行列に変換\n",
        "\n",
        "        # 報酬を設定し、与える\n",
        "        if done:\n",
        "            next_state = np.zeros(state.shape)  # 次の状態s_{t+1}はない\n",
        "            if t < 195:\n",
        "                reward = -1  # 報酬クリッピング、報酬は1, 0, -1に固定\n",
        "            else:\n",
        "                reward = 1  # 立ったまま195step超えて終了時は報酬\n",
        "        else:\n",
        "            reward = 0  # 各ステップで立ってたら報酬追加（はじめからrewardに1が入っているが、明示的に表す）\n",
        "\n",
        "        episode_reward += 1 # reward  # 合計報酬を更新\n",
        "\n",
        "        memory.add((state, action, reward, next_state))     # メモリの更新する\n",
        "        state = next_state  # 状態更新\n",
        "\n",
        "\n",
        "        # Qネットワークの重みを学習・更新する replay\n",
        "        if (memory.len() > batch_size) and not islearned:\n",
        "            mainQN.replay(memory, batch_size, gamma, targetQN)\n",
        "\n",
        "        if DQN_MODE:\n",
        "        # 2018.06.12\n",
        "        # shiglayさんさんより間違いを修正いただきました\n",
        "        # targetQN = mainQN   # 行動決定と価値計算のQネットワークをおなじにする\n",
        "        # ↓\n",
        "            targetQN.model.set_weights(mainQN.model.get_weights())\n",
        "\n",
        "        # 1施行終了時の処理\n",
        "        if done:\n",
        "            total_reward_vec = np.hstack((total_reward_vec[1:], episode_reward))  # 報酬を記録\n",
        "            print('%d Episode finished after %f time steps / mean %f' % (episode, t + 1, total_reward_vec.mean()))\n",
        "            break\n",
        "\n",
        "    # 複数施行の平均報酬で終了を判断\n",
        "    if total_reward_vec.mean() >= goal_average_reward:\n",
        "        print('Episode %d train agent successfuly!' % episode)\n",
        "        islearned = 1\n",
        "        if isrender == 0:   # 学習済みフラグを更新\n",
        "            isrender = 1\n",
        "\n",
        "            # env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合\n",
        "            # 10エピソードだけでどんな挙動になるのか見たかったら、以下のコメントを外す\n",
        "            # if episode>10:\n",
        "            #    if isrender == 0:\n",
        "            #        env = wrappers.Monitor(env, './movie/cartpole-experiment-1') #動画保存する場合\n",
        "            #        isrender = 1\n",
        "            #    islearned=1;"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-af720bccc4d3>:21: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "0 Episode finished after 23.000000 time steps / mean 2.300000\n",
            "1 Episode finished after 9.000000 time steps / mean 3.200000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "2 Episode finished after 22.000000 time steps / mean 5.400000\n",
            "3 Episode finished after 15.000000 time steps / mean 6.900000\n",
            "4 Episode finished after 11.000000 time steps / mean 8.000000\n",
            "5 Episode finished after 11.000000 time steps / mean 9.100000\n",
            "6 Episode finished after 10.000000 time steps / mean 10.100000\n",
            "7 Episode finished after 9.000000 time steps / mean 11.000000\n",
            "8 Episode finished after 8.000000 time steps / mean 11.800000\n",
            "9 Episode finished after 13.000000 time steps / mean 13.100000\n",
            "10 Episode finished after 9.000000 time steps / mean 11.700000\n",
            "11 Episode finished after 10.000000 time steps / mean 11.800000\n",
            "12 Episode finished after 10.000000 time steps / mean 10.600000\n",
            "13 Episode finished after 9.000000 time steps / mean 10.000000\n",
            "14 Episode finished after 9.000000 time steps / mean 9.800000\n",
            "15 Episode finished after 25.000000 time steps / mean 11.200000\n",
            "16 Episode finished after 9.000000 time steps / mean 11.100000\n",
            "17 Episode finished after 9.000000 time steps / mean 11.100000\n",
            "18 Episode finished after 10.000000 time steps / mean 11.300000\n",
            "19 Episode finished after 9.000000 time steps / mean 10.900000\n",
            "20 Episode finished after 8.000000 time steps / mean 10.800000\n",
            "21 Episode finished after 9.000000 time steps / mean 10.700000\n",
            "22 Episode finished after 9.000000 time steps / mean 10.600000\n",
            "23 Episode finished after 9.000000 time steps / mean 10.600000\n",
            "24 Episode finished after 9.000000 time steps / mean 10.600000\n",
            "25 Episode finished after 20.000000 time steps / mean 10.100000\n",
            "26 Episode finished after 25.000000 time steps / mean 11.700000\n",
            "27 Episode finished after 9.000000 time steps / mean 11.700000\n",
            "28 Episode finished after 21.000000 time steps / mean 12.800000\n",
            "29 Episode finished after 11.000000 time steps / mean 13.000000\n",
            "30 Episode finished after 10.000000 time steps / mean 13.200000\n",
            "31 Episode finished after 16.000000 time steps / mean 13.900000\n",
            "32 Episode finished after 25.000000 time steps / mean 15.500000\n",
            "33 Episode finished after 9.000000 time steps / mean 15.500000\n",
            "34 Episode finished after 9.000000 time steps / mean 15.500000\n",
            "35 Episode finished after 8.000000 time steps / mean 14.300000\n",
            "36 Episode finished after 26.000000 time steps / mean 14.400000\n",
            "37 Episode finished after 9.000000 time steps / mean 14.400000\n",
            "38 Episode finished after 10.000000 time steps / mean 13.300000\n",
            "39 Episode finished after 8.000000 time steps / mean 13.000000\n",
            "40 Episode finished after 44.000000 time steps / mean 16.400000\n",
            "41 Episode finished after 8.000000 time steps / mean 15.600000\n",
            "42 Episode finished after 28.000000 time steps / mean 15.900000\n",
            "43 Episode finished after 8.000000 time steps / mean 15.800000\n",
            "44 Episode finished after 8.000000 time steps / mean 15.700000\n",
            "45 Episode finished after 26.000000 time steps / mean 17.500000\n",
            "46 Episode finished after 8.000000 time steps / mean 15.700000\n",
            "47 Episode finished after 10.000000 time steps / mean 15.800000\n",
            "48 Episode finished after 9.000000 time steps / mean 15.700000\n",
            "49 Episode finished after 8.000000 time steps / mean 15.700000\n",
            "50 Episode finished after 8.000000 time steps / mean 12.100000\n",
            "51 Episode finished after 33.000000 time steps / mean 14.600000\n",
            "52 Episode finished after 8.000000 time steps / mean 12.600000\n",
            "53 Episode finished after 42.000000 time steps / mean 16.000000\n",
            "54 Episode finished after 7.000000 time steps / mean 15.900000\n",
            "55 Episode finished after 9.000000 time steps / mean 14.200000\n",
            "56 Episode finished after 9.000000 time steps / mean 14.300000\n",
            "57 Episode finished after 9.000000 time steps / mean 14.200000\n",
            "58 Episode finished after 9.000000 time steps / mean 14.200000\n",
            "59 Episode finished after 8.000000 time steps / mean 14.200000\n",
            "60 Episode finished after 29.000000 time steps / mean 16.300000\n",
            "61 Episode finished after 8.000000 time steps / mean 13.800000\n",
            "62 Episode finished after 9.000000 time steps / mean 13.900000\n",
            "63 Episode finished after 13.000000 time steps / mean 11.000000\n",
            "64 Episode finished after 25.000000 time steps / mean 12.800000\n",
            "65 Episode finished after 28.000000 time steps / mean 14.700000\n",
            "66 Episode finished after 25.000000 time steps / mean 16.300000\n",
            "67 Episode finished after 7.000000 time steps / mean 16.100000\n",
            "68 Episode finished after 31.000000 time steps / mean 18.300000\n",
            "69 Episode finished after 38.000000 time steps / mean 21.300000\n",
            "70 Episode finished after 8.000000 time steps / mean 19.200000\n",
            "71 Episode finished after 23.000000 time steps / mean 20.700000\n",
            "72 Episode finished after 8.000000 time steps / mean 20.600000\n",
            "73 Episode finished after 7.000000 time steps / mean 20.000000\n",
            "74 Episode finished after 25.000000 time steps / mean 20.000000\n",
            "75 Episode finished after 22.000000 time steps / mean 19.400000\n",
            "76 Episode finished after 8.000000 time steps / mean 17.700000\n",
            "77 Episode finished after 9.000000 time steps / mean 17.900000\n",
            "78 Episode finished after 8.000000 time steps / mean 15.600000\n",
            "79 Episode finished after 9.000000 time steps / mean 12.700000\n",
            "80 Episode finished after 22.000000 time steps / mean 14.100000\n",
            "81 Episode finished after 32.000000 time steps / mean 15.000000\n",
            "82 Episode finished after 9.000000 time steps / mean 15.100000\n",
            "83 Episode finished after 23.000000 time steps / mean 16.700000\n",
            "84 Episode finished after 10.000000 time steps / mean 15.200000\n",
            "85 Episode finished after 44.000000 time steps / mean 17.400000\n",
            "86 Episode finished after 39.000000 time steps / mean 20.500000\n",
            "87 Episode finished after 28.000000 time steps / mean 22.400000\n",
            "88 Episode finished after 9.000000 time steps / mean 22.500000\n",
            "89 Episode finished after 8.000000 time steps / mean 22.400000\n",
            "90 Episode finished after 9.000000 time steps / mean 21.100000\n",
            "91 Episode finished after 11.000000 time steps / mean 19.000000\n",
            "92 Episode finished after 8.000000 time steps / mean 18.900000\n",
            "93 Episode finished after 54.000000 time steps / mean 22.000000\n",
            "94 Episode finished after 9.000000 time steps / mean 21.900000\n",
            "95 Episode finished after 8.000000 time steps / mean 18.300000\n",
            "96 Episode finished after 38.000000 time steps / mean 18.200000\n",
            "97 Episode finished after 27.000000 time steps / mean 18.100000\n",
            "98 Episode finished after 8.000000 time steps / mean 18.000000\n",
            "99 Episode finished after 46.000000 time steps / mean 21.800000\n",
            "100 Episode finished after 47.000000 time steps / mean 25.600000\n",
            "101 Episode finished after 8.000000 time steps / mean 25.300000\n",
            "102 Episode finished after 29.000000 time steps / mean 27.400000\n",
            "103 Episode finished after 9.000000 time steps / mean 22.900000\n",
            "104 Episode finished after 8.000000 time steps / mean 22.800000\n",
            "105 Episode finished after 9.000000 time steps / mean 22.900000\n",
            "106 Episode finished after 9.000000 time steps / mean 20.000000\n",
            "107 Episode finished after 7.000000 time steps / mean 18.000000\n",
            "108 Episode finished after 9.000000 time steps / mean 18.100000\n",
            "109 Episode finished after 83.000000 time steps / mean 21.800000\n",
            "110 Episode finished after 8.000000 time steps / mean 17.900000\n",
            "111 Episode finished after 64.000000 time steps / mean 23.500000\n",
            "112 Episode finished after 35.000000 time steps / mean 24.100000\n",
            "113 Episode finished after 46.000000 time steps / mean 27.800000\n",
            "114 Episode finished after 20.000000 time steps / mean 29.000000\n",
            "115 Episode finished after 8.000000 time steps / mean 28.900000\n",
            "116 Episode finished after 23.000000 time steps / mean 30.300000\n",
            "117 Episode finished after 38.000000 time steps / mean 33.400000\n",
            "118 Episode finished after 9.000000 time steps / mean 33.400000\n",
            "119 Episode finished after 39.000000 time steps / mean 29.000000\n",
            "120 Episode finished after 29.000000 time steps / mean 31.100000\n",
            "121 Episode finished after 9.000000 time steps / mean 25.600000\n",
            "122 Episode finished after 36.000000 time steps / mean 25.700000\n",
            "123 Episode finished after 9.000000 time steps / mean 22.000000\n",
            "124 Episode finished after 24.000000 time steps / mean 22.400000\n",
            "125 Episode finished after 46.000000 time steps / mean 26.200000\n",
            "126 Episode finished after 8.000000 time steps / mean 24.700000\n",
            "127 Episode finished after 40.000000 time steps / mean 24.900000\n",
            "128 Episode finished after 35.000000 time steps / mean 27.500000\n",
            "129 Episode finished after 22.000000 time steps / mean 25.800000\n",
            "130 Episode finished after 8.000000 time steps / mean 23.700000\n",
            "131 Episode finished after 23.000000 time steps / mean 25.100000\n",
            "132 Episode finished after 25.000000 time steps / mean 24.000000\n",
            "133 Episode finished after 23.000000 time steps / mean 25.400000\n",
            "134 Episode finished after 24.000000 time steps / mean 25.400000\n",
            "135 Episode finished after 25.000000 time steps / mean 23.300000\n",
            "136 Episode finished after 8.000000 time steps / mean 23.300000\n",
            "137 Episode finished after 7.000000 time steps / mean 20.000000\n",
            "138 Episode finished after 8.000000 time steps / mean 17.300000\n",
            "139 Episode finished after 34.000000 time steps / mean 18.500000\n",
            "140 Episode finished after 20.000000 time steps / mean 19.700000\n",
            "141 Episode finished after 34.000000 time steps / mean 20.800000\n",
            "142 Episode finished after 17.000000 time steps / mean 20.000000\n",
            "143 Episode finished after 24.000000 time steps / mean 20.100000\n",
            "144 Episode finished after 10.000000 time steps / mean 18.700000\n",
            "145 Episode finished after 9.000000 time steps / mean 17.100000\n",
            "146 Episode finished after 7.000000 time steps / mean 17.000000\n",
            "147 Episode finished after 24.000000 time steps / mean 18.700000\n",
            "148 Episode finished after 19.000000 time steps / mean 19.800000\n",
            "149 Episode finished after 8.000000 time steps / mean 17.200000\n",
            "150 Episode finished after 8.000000 time steps / mean 16.000000\n",
            "151 Episode finished after 9.000000 time steps / mean 13.500000\n",
            "152 Episode finished after 8.000000 time steps / mean 12.600000\n",
            "153 Episode finished after 9.000000 time steps / mean 11.100000\n",
            "154 Episode finished after 27.000000 time steps / mean 12.800000\n",
            "155 Episode finished after 8.000000 time steps / mean 12.700000\n",
            "156 Episode finished after 18.000000 time steps / mean 13.800000\n",
            "157 Episode finished after 16.000000 time steps / mean 13.000000\n",
            "158 Episode finished after 8.000000 time steps / mean 11.900000\n",
            "159 Episode finished after 18.000000 time steps / mean 12.900000\n",
            "160 Episode finished after 8.000000 time steps / mean 12.900000\n",
            "161 Episode finished after 9.000000 time steps / mean 12.900000\n",
            "162 Episode finished after 8.000000 time steps / mean 12.900000\n",
            "163 Episode finished after 29.000000 time steps / mean 14.900000\n",
            "164 Episode finished after 9.000000 time steps / mean 13.100000\n",
            "165 Episode finished after 25.000000 time steps / mean 14.800000\n",
            "166 Episode finished after 9.000000 time steps / mean 13.900000\n",
            "167 Episode finished after 26.000000 time steps / mean 14.900000\n",
            "168 Episode finished after 10.000000 time steps / mean 15.100000\n",
            "169 Episode finished after 8.000000 time steps / mean 14.100000\n",
            "170 Episode finished after 17.000000 time steps / mean 15.000000\n",
            "171 Episode finished after 15.000000 time steps / mean 15.600000\n",
            "172 Episode finished after 9.000000 time steps / mean 15.700000\n",
            "173 Episode finished after 8.000000 time steps / mean 13.600000\n",
            "174 Episode finished after 9.000000 time steps / mean 13.600000\n",
            "175 Episode finished after 8.000000 time steps / mean 11.900000\n",
            "176 Episode finished after 26.000000 time steps / mean 13.600000\n",
            "177 Episode finished after 7.000000 time steps / mean 11.700000\n",
            "178 Episode finished after 8.000000 time steps / mean 11.500000\n",
            "179 Episode finished after 8.000000 time steps / mean 11.500000\n",
            "180 Episode finished after 33.000000 time steps / mean 13.100000\n",
            "181 Episode finished after 9.000000 time steps / mean 12.500000\n",
            "182 Episode finished after 9.000000 time steps / mean 12.500000\n",
            "183 Episode finished after 9.000000 time steps / mean 12.600000\n",
            "184 Episode finished after 9.000000 time steps / mean 12.600000\n",
            "185 Episode finished after 9.000000 time steps / mean 12.700000\n",
            "186 Episode finished after 9.000000 time steps / mean 11.000000\n",
            "187 Episode finished after 8.000000 time steps / mean 11.100000\n",
            "188 Episode finished after 8.000000 time steps / mean 11.100000\n",
            "189 Episode finished after 18.000000 time steps / mean 12.100000\n",
            "190 Episode finished after 8.000000 time steps / mean 9.600000\n",
            "191 Episode finished after 8.000000 time steps / mean 9.500000\n",
            "192 Episode finished after 8.000000 time steps / mean 9.400000\n",
            "193 Episode finished after 30.000000 time steps / mean 11.500000\n",
            "194 Episode finished after 8.000000 time steps / mean 11.400000\n",
            "195 Episode finished after 9.000000 time steps / mean 11.400000\n",
            "196 Episode finished after 8.000000 time steps / mean 11.300000\n",
            "197 Episode finished after 8.000000 time steps / mean 11.300000\n",
            "198 Episode finished after 33.000000 time steps / mean 13.800000\n",
            "199 Episode finished after 8.000000 time steps / mean 12.800000\n",
            "200 Episode finished after 18.000000 time steps / mean 13.800000\n",
            "201 Episode finished after 17.000000 time steps / mean 14.700000\n",
            "202 Episode finished after 11.000000 time steps / mean 15.000000\n",
            "203 Episode finished after 20.000000 time steps / mean 14.000000\n",
            "204 Episode finished after 8.000000 time steps / mean 14.000000\n",
            "205 Episode finished after 31.000000 time steps / mean 16.200000\n",
            "206 Episode finished after 26.000000 time steps / mean 18.000000\n",
            "207 Episode finished after 15.000000 time steps / mean 18.700000\n",
            "208 Episode finished after 7.000000 time steps / mean 16.100000\n",
            "209 Episode finished after 9.000000 time steps / mean 16.200000\n",
            "210 Episode finished after 21.000000 time steps / mean 16.500000\n",
            "211 Episode finished after 9.000000 time steps / mean 15.700000\n",
            "212 Episode finished after 27.000000 time steps / mean 17.300000\n",
            "213 Episode finished after 9.000000 time steps / mean 16.200000\n",
            "214 Episode finished after 28.000000 time steps / mean 18.200000\n",
            "215 Episode finished after 8.000000 time steps / mean 15.900000\n",
            "216 Episode finished after 20.000000 time steps / mean 15.300000\n",
            "217 Episode finished after 30.000000 time steps / mean 16.800000\n",
            "218 Episode finished after 20.000000 time steps / mean 18.100000\n",
            "219 Episode finished after 9.000000 time steps / mean 18.100000\n",
            "220 Episode finished after 10.000000 time steps / mean 17.000000\n",
            "221 Episode finished after 27.000000 time steps / mean 18.800000\n",
            "222 Episode finished after 27.000000 time steps / mean 18.800000\n",
            "223 Episode finished after 8.000000 time steps / mean 18.700000\n",
            "224 Episode finished after 34.000000 time steps / mean 19.300000\n",
            "225 Episode finished after 31.000000 time steps / mean 21.600000\n",
            "226 Episode finished after 27.000000 time steps / mean 22.300000\n",
            "227 Episode finished after 21.000000 time steps / mean 21.400000\n",
            "228 Episode finished after 23.000000 time steps / mean 21.700000\n",
            "229 Episode finished after 15.000000 time steps / mean 22.300000\n",
            "230 Episode finished after 9.000000 time steps / mean 22.200000\n",
            "231 Episode finished after 8.000000 time steps / mean 20.300000\n",
            "232 Episode finished after 14.000000 time steps / mean 19.000000\n",
            "233 Episode finished after 9.000000 time steps / mean 19.100000\n",
            "234 Episode finished after 19.000000 time steps / mean 17.600000\n",
            "235 Episode finished after 13.000000 time steps / mean 15.800000\n",
            "236 Episode finished after 8.000000 time steps / mean 13.900000\n",
            "237 Episode finished after 17.000000 time steps / mean 13.500000\n",
            "238 Episode finished after 16.000000 time steps / mean 12.800000\n",
            "239 Episode finished after 14.000000 time steps / mean 12.700000\n",
            "240 Episode finished after 9.000000 time steps / mean 12.700000\n",
            "241 Episode finished after 26.000000 time steps / mean 14.500000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-af720bccc4d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Qネットワークの重みを学習・更新する replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mislearned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mmainQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetQN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDQN_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-af720bccc4d3>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, memory, batch_size, gamma, targetQN)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_b\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtargetQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_b\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Qネットワークの出力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction_b\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m               \u001b[0;31m# 教師信号\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;31m# In this case, we run extensive shape validation checks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m             \u001b[0mfeed_input_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             \u001b[0mfeed_input_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;31m# Standardize the inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}